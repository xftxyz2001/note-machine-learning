## 1、想想你上大学期间之前用传统封闭式编程解决不了的问题是否可以用机器学习方法解决？或者说谈谈你想用机器学习技术来解决一个什么样问题？

需要进行大量手工调整或需要拥有长串规则才能解决的问题可以使用机器学习算法简化代码、提高性能。

使用传统方法难以解决的复杂问题，如数据挖掘、自然语言处理，计算机视觉、语音识别。推荐系统


## 2、用Xmind或其它工具完成一张“机器学习的业务应用”图。

### 1正则化算法

正则化是一种降低过拟合风险的方法，本质是在模型训练的过程中，不一味的降低损失函数，追求偏差的降低，而是在损失函数中加入“正则化惩罚项”，常见的有L1、L2正则化惩罚项。惩罚项保证了模型方差与偏差之间的平衡，增加模型的方差，以确保模型的泛化性。

适用场合：模型结构复杂，数据量少。

缺点：增加欠拟合风险，难以校准（L1、L2正则化惩罚项的系数需要手动调节）。

Ps.

L0正则化的值是模型参数中非零参数的个数。

L1正则化表示各个参数绝对值之和。L0与L1可以让特征参数变得稀疏，利于特征选择。（Lasso）

L2正则化标识各个参数的平方和。L2让特征参数变得均匀。（Ridge）

正则化惩罚项带来的附加作用就是让特征之间的相关性降低，L1是将类似的特征参数变得只剩一个，L2是将具有强相关性的两个特征的参数变的类似，从而模型的稳定性得到提升。

### 2 集成学习（boosting+bagging+RF）

集成学习是多个弱学习器组合成为一个强学习器的方法。一般弱学习器集成的方法有两种，一种是boosting，一种是bagging。

Boosting中的弱学习器是通过串行的方式连接的，每个学习器都和其前一个学习器有依赖关系。比如Adaboost，用于二分类的集成学习算法，采用了重赋权的方式，每个基学习器的输入数据将根据其上一个学习器的预测结果来决定，预测错误的样本权值将被提高，预测正确的样本权值将被降低。最后将每个弱学习器的预测结果进行加权相加作为最后的输出结果。

优点：Boosting更注重bias的降低，所以可以将泛化性弱的学习器组合成一个强学习器。

缺点：稳定性不佳。

Bagging中的弱学习器是通过并行的方式连接的，各个学习器之间没有依赖关系，采用有放回的采样得到每个弱学习器的样本集。最后结果是通过投票法、平均法、

优点：Bagging更注重得到泛化性强的学习器。

缺点：需要大量维护工作，Bagging的效果和参数的选择关系比较大，用默认参数往往没有很好的效果。

而随机森林是bagging的一种升级，不仅每个决策树使用的数据集是抽样得到的，其中使用的样本特征也要通过抽样。由于每个树是独立的，并且有袋外数据（0.368），所以泛化性强。

优点：不容易过拟合，默认参配置就能得到不错的效果。随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。

### 3决策树

决策树是用树形结构来对问题进行层层剖析的算法，主要结构中有根节点、内部节点与叶节点。叶节点是决策结果，非叶节点是对某一种属性的测试。

有三种较为著名的决策树：

ID3决策树使用的分支划分策略是信息增益，该数值表示选择该属性对树进行分支后的系统纯度上升最高，数值越大表示纯度上升越大。该指标对于样本数量多的属性有偏好。

C4.5使用的信息增益率，该指标对于样本数量少的属性有偏好。

CART决策树使用的是基尼系数（gini_index），基尼系数是指从样本集中同时抽取两个样本，两个样本类型不同的概率。

优点：易于解释，最后结果树可以看到。

缺点：趋向过拟合，容易陷入局部最小值，没有在线学习。数据集有增量后决策树需要重建。

实用场景：因为它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，数据分析师希望更好的理解手上的数据的时候往往可以使用决策树。同时它也是相对容易被攻击的分类器。这里的攻击是指人为的改变一些特征，使得分类器判断错误。常见于垃圾邮件躲避检测中。因为决策树最终在底层判断是基于单个条件的，攻击者往往只需要改变很少的特征就可以逃过监测。受限于它的简单性，决策树更大的用处是作为一些更有用的算法的基石。

### 4 基于实例的算法

典型的例子是KNN，它的思路是对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型，它的特点是完全跟着数据走，没有数学模型可言。

适用情景：需要一个特别容易解释的模型的时候，比如需要向用户解释原因的推荐算法。

### 5 贝叶斯算法

典型的例子是朴素贝叶斯，核心思路是根据条件概率计算待判断点的类型。它是高偏差低方差的模型，因为它简单的假设了各个数据之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会Bias部分大于Variance部分，也就是说高偏差而低方差。

优点：①速度快。朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已；对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）；

如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。

②对缺失数据不太敏感，算法也比较简单，常用于文本分类；朴素贝叶斯对结果解释容易理解。

缺点：需要计算先验概率；分类决策存在错误率；对输入数据的表达形式很敏感；由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好；

朴素贝叶斯应用领域：目前还在垃圾邮件分类中被使用，欺诈检测，文本分析：文本分类。文本情感分析。

### 6 回归算法  Logistic Regression（逻辑回归）+线性回归

逻辑回归属于判别式模型，相当于把线性回归的结果通过softmax函数映射到了【0,1】区间。同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且不必像在用朴素贝叶斯担心特征是否相关，并且使用在线梯度下降算法-online gradient descent可以轻松地利用新数据来更新模型。

优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；计算代价不高，易于理解和实现；

缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高；不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类，也可以使用一对多分类。），且必须线性可分；对于非线性特征，需要进行转换。

logistic回归应用领域：用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。信用评估，测量市场营销的成功度，预测某个产品的收益，特定的某天是否会发生地震。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。

线性回归，优点： 实现简单，计算简单；缺点： 不能拟合非线性数据.

### 7 SVM支持向量机

原理参考：

https://blog.csdn.net/weixin_41090915/article/details/79177267

支持向量机，寻找空间中间隔最大的分割平面，该分割平面可以尽可能的分离不同类别的数据。而离这个分割平面边界最近的点就是支持向量。本质是求min||w||。

优点：SVM具有高准确率，强泛化能力，因为过拟合的意思是训练结果跟着全体数据走，但是SVM只侧重于支持向量，无需依赖整个数据。可以解决小样本下机器学习问题；

而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。能够处理非线性特征的相互作用。

无局部极小值问题；（相对于神经网络等算法）

缺点：

内存消耗大，当观测样本很多时，效率并不是很高；

对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；

对于核函数的高维映射解释力不强，尤其是径向基函数。常规SVM只支持二分类；

对缺失数据敏感；SVM运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。

SVM应用领域：文本分类、图像识别（主要二分类领域，毕竟常规SVM只能解决二分类问题）

7.1 SVM核函数

主要参考：https://blog.csdn.net/v_JULY_v/article/details/7624837

对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF径向基函数、以及sigmoid核）：

第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；

第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；

第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。

### 8 降维算法

无监督：PCA

有监督：LDA 让同类的数据尽可能投射在一起，不同类的数据尽可能的分开。

适用场景：线性问题

缺点：非线性问题难以解决。不过可以使用基于核的PCA。

### 9 神经网络

优点：在语音、视觉、游戏任务中表现的极好，算法可以快速的调整。

缺点：是需要大量的数据支持，内部黑箱状态，可解释性差。

### 10 非均值算法

原理：设定数据中k个中心，每次迭代都计算样本数据，就计算这个样本数据到这k个中心的距离，距离最短的为一类，然后通过均值法重新设定每一类数据的中心。聚类满足：同一聚类中的对象相似度较高，而不同聚类中的对象相似度较低。

适用场合：聚类，K-means算法通常可以应用于维数、数值都很小且连续的数据集，比如：从随机分布的事物集合中将相同事物进行分组。



## 3、用“图”的方式总结一下第一章绪论中的知识点。

### 1.1 机器学习的定义
正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。

另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：

P：计算机程序在某任务类T上的性能。
T：计算机程序希望实现的任务类。
E：表示经验，即历史的数据集。
若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。

### 1.2 机器学习的一些基本术语
假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：

所有记录的集合为：数据集。

每一条记录为：一个实例（instance）或样本（sample）。

例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。

对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。

一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。

计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：

所有训练样本的集合为：训练集（trainning set），[特殊]。

所有测试样本的集合为：测试集（test set），[一般]。

机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。

西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：

预测值为离散值的问题为：分类（classification）。

预测值为连续值的问题为：回归（regression）。

我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：

训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。

训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。
