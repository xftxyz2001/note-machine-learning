## 1、试分析简单序列爬虫、并发爬虫、通用爬虫及主题爬虫的联系与区别。并分别给出其基本算法框架。
### 通用网络爬虫

**通用网络爬虫又称全网爬虫（Scalable Web Crawler）**，**爬行对象从一些种子 URL 扩充到整个 Web，主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。**

**这类网络爬虫的爬行范围和数量巨大，对于****爬行速度和存储空间要求较高****，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式，但需要较长时间才能刷新一次页面。**

简单的说就是互联网上抓取所有数据。

### ​​​​​​​聚焦网络爬虫

**聚焦网络爬虫（Focused Crawler），又称主题网络爬虫（Topical Crawler）**，是指**选择性地爬行那些与预先定义好的主题相关页面的网络爬虫。**

和通用网络爬虫相比，聚焦爬虫只需要爬行与主题相关的页面，**极大地节省了硬件和网络资源，保存的页面也由于数量少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求 。**

简单的说就是互联网上只抓取某一种数据。

### ​​​​​​​增量式网络爬虫

**增量式网络爬虫（Incremental Web Crawler）** 是 指 **对 已 下 载 网 页 采 取 增量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。**

和周期性爬行和刷新页面的网络爬虫相比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面 ，**并不重新下载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。**

简单的说就是互联网上只抓取刚刚更新的数据。

### ​​​​​​​Deep Web 爬虫

Web 页面按存在方式可以分为**表层网页（Surface Web）和深层网页（Deep Web，也称 Invisible Web Pages 或 Hidden Web）。**

**表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的 Web 页面。**

Deep Web 是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的 Web 页面。


## 2、网页爬取数据的主要预处理包括哪些步骤？何谓网页解析？主要包括哪些内容？
缺失值处理：删除记录，数据插补， 不处理。

常用的数据插补方法：
- 均值，中位数，众数插补
- 使用固定值
- 回归方法（预测）
- 插值法（拉格朗日插值法，牛顿插值法等）

异常值处理
- 剔除含有异常值的记录
- 视为缺失项
- 平均值修正
- 不处理

是来自某些网站搜集来的资料，这些资料包含编号，名称，图片，链接等等元素，在这里只是将信息从标签中提取出来

XPath、Re、CSS、BS4


## 3、对爬虫爬取的网页主要的组织方式有哪些形式？各有何利弊？
1、线性结构
这是网站最简单的一种结构，它是以某种顺序组织的，可以是时间顺序，也可以是逻辑甚至是字母顺序。通过这些顺序呈线性地链接。如一般的索引就采用线性结构。线性结构是组织网页的基本结构，复杂的结构也可以看成是由线性结构组成的。

2、网状结构
这是最复杂的组织结构，它完全没有限制，网页组织自由链接。这种结构允许访问者从一个信息栏目跳到另一个栏目去，其目的就是充分利用网络资源和充分享受超级链接。整个互联网就是一个超级大的“网”状结构。

3、等级结构
等级结构由一条等级主线构成索引，每一个等级点又由一条线性结构构成。如网站导航等就是这种结构。在构造等级之前，你必须完全彻底的理解你的网站内容，避免线性组织不严的错误，不方便浏览者。 


## 4、衡量爬虫质量的标准有哪些常用的方式？
1. 需要在有限的资源内获取最想要的网页。这就需要考虑网页的重要性，更为准确地抓取想要的网页。
2. 所抓取网页的时效性。因为对于下载到本地的网页，可能源网页已经发生更新，则为确保所抓取网页的有效性，需要尽可能地保证网页的时新性。如Google的fresh Bot系统主要针对网页的时效性进行设计的，其可以达到每秒的更新周期。而对于更新不是那么频繁网页的抓取Google则有一套deep crawl Bot的抓取系统，以天为更新周期。
3. 在上面两者的基础上，力求使得抓取的网页更加广。


## 5、何谓爬虫道德？如何解决爬虫冲突？
robots协议全称是“网络爬虫排除标准”，它意在规范爬虫程序的行为。一个网站的robots协议就是在告诉
爬虫程序什么页面可以爬取，什么页面不可以爬虫，以及禁止某些爬虫程序等。不过robots协议并没有
形成法律的规范，仍然属于道德层面的约束。

### 1、UserAgent
UserAgent的设置能使服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本等信息。对于一些网站来说，它会检查我们发送的请求中所携带的UserAgent字段，如果非浏览器，就会被识别为爬虫，一旦被识别出来， 我们的爬虫也就无法正常爬取数据了。

#### 解决方法
收集常见的useragent作为配置文件，每次访问的时候取出一个作为头部发送请求，需要注意的是同一个useragent如果访问频率太高也有可能被识别出来而被禁止，因此可以设置随机选取的策略，每一次访问都随机选取一个。此外对于那些支持m端的网站，有时会根据useragent识别是否为移动端，如果是可能会自动跳转到移动端，如果此时你正在爬取的是web端的话就有可能解析出问题，所以需要注意。  我们可以使用第三方库：

```python
from fake_useragent import UserAgent
ua = UserAgent()
print(ua.random)
# Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36
```

### 2、IP限制
IP限制在反爬虫中是非常常见的，一般来讲都会有这个限制。可以想像如果单位时间内你的访问次数非常频繁，对于服务器的压力是个不小的开销，因此当网站服务器判定你的访问频率超过某个阈值了，服务器就会对你的IP进行限制，通常限制的策略也不一样，有的可能是限制访问一定的时间比如一天，一周等等，也有可能是永久地限制。

#### 解决方法
基于IP的反爬限制我们可以通过自己取搭建一个IP池，所谓IP池就是一个拥有很多IP的容器，这个容器可以用一个队列来实现或这其他数据结构。我们需要从一些免费的IP网站上去爬取这些IP，但是这些IP都是不太稳定的，所以我们也需要一个检测模块，从容器中获取IP并检测它是否有效，对于有效的IP我们把它放到容器中，对于无效的IP就从容器中剔除。随后通过暴露接口的方式来获取这些IP，用一个图来表示整个过程：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601205805587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlY2Nlbg==,size_16,color_FFFFFF,t_70#pic_center)  虽然自己搭建IP池是个不错的方法，但是要知道那些免费的IP稳定性是很差的，因此如果是在生产环境中，对数据的完整性要求很高那么是不建议取这么做的，一般我们还是采取使用付费的IP，例如像阿布云这类的付费IP，这些IP 的稳定性都是比较高的，因此在生产环境中经常使用。

### 3、验证码或模拟登陆
通过验证码来进行反爬取措施是比较常用的方法，特别是对一些要进行登陆的页面，在以前验证码还比较简单的时候可以通过一些简单的图像识别处理来通过，但是现在的验证码越来越复杂，并且已经达到了几乎迷惑人的识别的阶段，那么这个就不好处理了。

#### 解决方法
验证码识别可以采用一些简单的处理：二值化、中值滤波去噪、分割、紧缩重排（让高矮统一）、字库特征匹配识别。或者使用百度AI开放平台OCR技术。对于复杂的验证码就不要花时间去破解了，即使破解准确率也比较低，可以考虑借助于打码平台。对于模拟登陆我们可以采用维持一个会话的方式，这个也很简单，例如可以使用requests库来申明一个session对象，将我们登陆的信息以表单形式发送登陆后，通过这个会话发送get请求或者post请求。

### 4、Ajax动态加载
Ajax动态加载的原理就是当访问一个URL时，会先加载网页源代码，然后会在浏览器中执行Javascript代码，这些代码会加载很多东西，这些请求都属于AJAX请求，通过这些请求会生成很多的数据，然后再将这些数据传输到网页中。所以对这些请求，你直接访问URL是没有数据的。  因此Ajax技术通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。在现在的网站中被广泛采用。

#### 解决方法
对于使用动态加载的数据，我们只能通过调试工具或者抓包工具去分析Ajax请求的过程，然后去模拟这个请求发送来获取数据，一般我们获取到的数据都是json的数据格式，我们只需要解析json字符串就可以。

### 5、cookies限制
对于某些网站会要求登录后才能访问，并且会对同一个账号进行访问的速度进行限制，例如同一个账号下，单位时间内超过某个固定的阈值就会进行限制。

#### 解决方法
我们可以通过模拟登录然后维持一个会话去发送请求，但是当访问数据较多时，可以研究服务器对账号进行限制的规律，例如笔者曾经写过一个爬虫，当一个账号访问在某个时间段内的次数超过100次后，就会对这个账号进行限制一个小时，此时就必须通过验证码输入后方可访问或者等待一个小时后可以继续访问。但是当你访问次数不超过100次时，再过10分钟就可以再次访问100次。这个解决方法可以通过打码平台解决，但是笔者为了给公司节省支出，采用了多个账号使用时间片的方式解决。我先准备多个账号获取到cookies，然后携带这些cookies取访问，每个cookies值访问90个请求，当前的cookies次数达到90次后，就使用下一个，依次轮转，当轮转一圈后保证时间间隔大于10分钟即可。并取得了很好的效果。  当然还有一种方式就是搭建一个cookies池，动态得获取cookies，然后携带这些cookies去访问。

### 6、js加密
这种方式常见于小程序或者APP的数据爬取，一般在post请求中较多。例如在某个post请求中会携带一些参数，这些参数中的某个字段是一串变化的字符串，这些字符串的生成一般是通过body中的参数以某种方式组合后做一个md5加密然后发送给服务器。例如很多网站会采用的signature签名机制。

#### 解决方法
对于这种没有别的办法，只能分析js代码，看看它是如何构造这段字符串的，然后通过Python中的库去模拟js构造的过程，然后发送请求。对于移动端的请求，例如在爬取小程序，就需要你能够获取小程序中的js代码，一般获取到的小程序的js代码都是经过混乱处理的，例如全篇的a,b,c,d作为变量名和函数名，这个需要你找到直接相关的代码段去分析构造过程。  对于APP的话，没办法，只能通过一部root后的安卓手机，反编译apk，然后拿到反编译后的代码，由于安卓用Java进行开发，因此需要你具备一定的Java基础。此外，你拿到的源码并不是完整的源代码，在反编译处理过后会有很多，未能正常反编译的代码。例如笔者就遇到过在某一个构造字符串的方法中（这个方法就是构造加密字段的方法）未能反编译出源代码，这就需要你能够阅读字节码指令，从这些字节码指令中，翻译出执行过程，这就需要你有一定的JVM的知识，笔者记得当时花了好几天才破解。因此对于这部份就需要具备一定的逆向经验。

### 7、数据加密
有些通过ajax加载的json数据并不是json字符串，而是通过压缩或者进行了某种处理的，例如使用base64进行转换的数据。

#### 解决方法
对于这样的数据，我们可以通过Python中的一些库对压缩数据解压缩，但是你得知道这些数据是采用什么方式压缩的，一般可以去尝试用不同的方法解压缩看看哪种能得到正确的字符串。由于base64是可逆向的，如果使用这种方式直接逆向转换就行了。  爬虫中的反爬方式还是很多的，我这里只是列出了一些比较常见的方式以及解决方法。对于反爬虫和反反爬虫这对博弈的存在我们需要花时间和经历去研究。


## 6、结构化数据抽取的主要方法有什么？
映射规则分为点规则和边规则两种
点和边都支持ID配置、类型配置和任意数量的属性配置，其中ID和类型是必填项；对于边，还需要配置fromId和toId
ID配置支持2种：自动生成；数据字段值，支持多字段连接
实体名称支持2种：数据字段值（理论上也可以支持多字段连接）；字面值
类型配置支持3种：Schema选择；数据字段值；字面值
属性名配置支持2种：Schema选择；字面值（一般就是字段名）
属性值配置支持2种：数据字段值；字面值
fromId 和 toId，是边的头尾节点标识，一般采用数据字段值，与节点的ID对应，支持多字段连接
keyFields 标识字段，用于冲突检测；ID为空时，自动将keyFields作为ID
mustFields 必填字段，如果对应字段为空，则产生必填冲突
idPrefix 用于标识同一批数据；便于对数据维护管理，前端可以使用图谱ID或用户输入的标识符。


## 7、包装器归纳方法的主要原理是什么？
1、网页清洗：有些网页结构不规范，例如前后标签不对称，没有结束标签符。不规范的网页结构容易在抽取的过程中抽取的过程中产生噪声。清洗可以用tidy来完成。
2、网页标注：网页标注是在网页上标注你需要抽取数据的过程。标注的过程可以是给网页中的某个位置打上特殊的标签表明这是要抽取的数据。
3、包装器空间的生成：对标注的数据生成XPath集合空间，对生成的集合进行归纳，形成若干子集。归纳的规则是在子集中的XPath能够覆盖多个标注的数据项，具有一定的泛化能力。
4、包装器评估：准确率和召回率。

## 8、何谓列表页？详情页？
列表页：列表页可以查看和处理大量的条目，常有导航至详情的作用。用户可在列表页对条目进行筛选、对比、新增、分析、下钻至条目完整详情页等操作。

详情页：详情页向用户展示一个对象的完整信息，主要用与信息浏览，允许对该对象发起编辑等操作。


## 9、 在结构化数据抽取时，什么是地标？主要的通配符有哪些？
识别来自一个或多个数据源的数据，其中数据与至少一个结构化文档相关联。提取包含在至少一个结构化文档内的数据集，并且将一个或多个记录项添加到可搜索的数据库，其中一个或多个记录项对应于已提取的数据集。

星号（`*`）
可以使用星号代替零个、单个或多个字符。如果正在查找以AEW开头的一个文件，但不记得文件名其余部分，可以输入`AEW*`，查找以AEW开头的所有文件类型的文件，如AEWT.txt、AEWU.EXE、AEWI.dll等。要缩小范围可以输入`AEW*.txt`，查找以AEW开头的所有文件类型并.txt为扩展名的文件如AEWIP.txt、AEWDF.txt。
问号（`?`）
可以使用问号代替一个字符。如果输入love?，查找以love开头的一个字符结尾文件类型的文件，如lovey、lovei等。要缩小范围可以输入love?.doc，查找以love开头的一个字符结尾文件类型并.doc为扩展名的文件如lovey.doc、loveh.doc。
通配符包括星号“`*`”和问号“`?`”
星号表示匹配的数量不受限制，而后者的匹配字符数则受到限制。这个技巧主要用于英文搜索中，如输入““computer*”，就可以找到“computer、computers、computerised、computerized”等单词，而输入“comp?ter”，则只能找到“computer、compater、competer”等单词。


## 10、何谓提纯？ 主要的提纯方法有哪些？具体如何实现提纯？
Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。


---
- [CSDN-爬虫分类——通用网络爬虫、聚焦网络爬虫、增量式网络爬虫、深层网络爬虫](https://blog.csdn.net/qq_39368007/article/details/105047654)
- [CSDN-数据预处理包括哪几步？](https://blog.csdn.net/weixin_42159940/article/details/86376560)
- [CSDN-网页的四种解析方式](https://blog.csdn.net/qq_42796939/article/details/85254171)
- [CSDN-搜索引擎核心读书心得1：爬虫质量的3个标准](https://blog.csdn.net/ljp1919/article/details/47447561)
- [简书-爬虫：robots协议 -- 爬虫的道德约束](https://www.jianshu.com/p/4be48b3b089f)
- [CSDN-爬虫中的那些反爬虫措施以及解决方法](https://blog.csdn.net/Leccen/article/details/106480317)
- [Ant Design 5.0](https://ant.design/index-cn)
- [CSDN-知识图谱学习与实践（6）——从结构化数据进行知识抽取（D2RQ介绍）](https://blog.csdn.net/cooldream2009/article/details/98873281)
- [CSDN-包装器简介](https://blog.csdn.net/weixin_44602176/article/details/100839487)
- [CSDN-数据挖掘之特征提取](https://blog.csdn.net/gyh80239100/article/details/105000460)
