## 1.什么是支持向量。
在支持向量机中，距离超平面最近的且满足一定条件的几个训练样本点被称为支持向量。

支持向量机(support vector machines,SVM)是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。
除此之外，SVM算法还包括核函数，核函数可以使它成为非线性分类器。
在了解SVM算法之前，我们要先认识一下线性分类器。


线性分类器：假设在一个二维线性可分的数据集中，我们要找到一个超平面把两组数据分开，已知的方法有我们已经学过的线性回归和逻辑回归，这条直线可以有很多种，如下图的H1、H2、H3哪一条直线的效果最好呢，也就是说哪条直线可以使两类的空间大小相隔最大呢？

我们凭直观感受应该觉得答案是H3。
首先H1不能把类别分开，这个分类器肯定是不行的；H2可以，但分割线与最近的数据点只有很小的间隔，如果测试数据有一些噪声的话可能就会被H2错误分类(即对噪声敏感、泛化能力弱)。
H3以较大间隔将它们分开，这样就能容忍测试数据的一些噪声而正确分类，是一个泛化能力不错的分类器。
因此我们把这个划分数据的决策边界就叫做超平面。
离这个超平面最近的点就是”支持向量”，点到超平面的距离叫做间隔，支持向量机的意思就是使超平面和支持向量之间的间隔尽可能的大，这样才可以使两类样本准确地分开。


## 2.什么是特征空间和输入空间。
- 输入空间(input space)：输入所有可能取值的**集合**
- 输出空间(output space)：输出所有可能取值的**集合**  
    输入与输出空间可以是有限元素的集合，也可以是整个欧式空间（定义了內积的实线性空间），它们可以是同一个空间，也可以是不同的空间，通常，输出空间远远小于输入空间。
- 特征空间(feature space)：所有**特征向量**存在的空间。每个具体的输入是一个实例，通常由特征向量表示。  
    例如：输入实例x的特征向量记作 $x = ( x ( 1 ) , x ( 2 ) , … , x ( i ) , … , x ( n ) ) T x=(x^{(1)},x^{(2)},…,x^{(i)},…,x^{(n)})^T x\=(x(1),x(2),…,x(i),…,x(n))T$
    其中，x<sup>(i)</sup>表示x的第i个特征向量，通常用x<sub>i</sub>表示多个输入变量中的第i个,多个这样的特征向量组成的空间就叫特征空间。
- 假设空间(hypothesis space)：输入空间到输出空间映射的集合。这一映射由模型来表示，换句话说，这是学习模型的集合


## 3.简述支持向量机的原理。
SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示， 
$w⋅x+b=0 \boldsymbol{w}\cdot x+b=0 \boldsymbol{w}\cdot x+b=0$
即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示， $ \boldsymbol{w}\cdot x+b=0 $ 即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

![](https://pic1.zhimg.com/v2-197913c461c1953c30b804b4a7eddfcc_r.jpg)

## 4.尝试从线性支持向量机的原始目标函数推导出拉格朗日变换后的目标函数及其对偶问题。
在推导之前，先给出一些定义。假设给定一个特征空间上的训练数据集

$$
 T=\left\{ \left( \boldsymbol{x}_1,y_1 \right) ,\left( \boldsymbol{x}_2,y_2 \right) ,...,\left( \boldsymbol{x}_N,y_N \right) \right\} 
$$

其中， $\boldsymbol{x}_i\in \mathbb{R}^n $ ， $ y_i\in \left\{ +1,-1 \right\} ,i=1,2,...N $ ， $x_i $ 为第 $ i $ 个特征向量， $ y_i $ 为类标记，当它等于+1时为正例；为-1时为负例。再假设训练数据集是**线性可分**的。

** 几何间隔**：对于给定的数据集 $ T $ 和超平面 $w\cdot x+b=0$ ，定义超平面关于样本点 $ \left( x_i,y_i \right) $ 的几何间隔为

$$
 \gamma _i=y_i\left( \frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert}\cdot \boldsymbol{x}_{\boldsymbol{i}}+\frac{b}{\lVert \boldsymbol{w} \rVert} \right) 
$$

超平面关于所有样本点的几何间隔的最小值为

$$
 \gamma =\underset{i=1,2...,N}{\min}\gamma _i 
$$

实际上这个距离就是我们所谓的**支持向量**到超平面的距离。

根据以上定义，SVM模型的求解最大分割超平面问题可以表示为以下约束最优化问题

$$
 \underset{\boldsymbol{w,}b}{\max}\ \gamma 
$$

$$
 s.t.\ \ \ y_i\left( \frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert}\cdot \boldsymbol{x}_{\boldsymbol{i}}+\frac{b}{\lVert \boldsymbol{w} \rVert} \right) \ge \gamma \ ,i=1,2,...,N 
$$

将约束条件两边同时除以 $ \gamma $ ，得到

$$
 y_i\left( \frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert \gamma}\cdot \boldsymbol{x}_{\boldsymbol{i}}+\frac{b}{\lVert \boldsymbol{w} \rVert \gamma} \right) \ge 1 
$$

因为 $ \lVert \boldsymbol{w} \rVert \text{，}\gamma $ 都是标量，所以为了表达式简洁起见，令

$$
\boldsymbol{w}=\frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert \gamma}
$$

$$
b=\frac{b}{\lVert \boldsymbol{w} \rVert \gamma} 
$$

得到

$$
y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \ge 1,\ i=1,2,...,N
$$

又因为最大化 $ \gamma $ ，等价于最大化 $ \frac{1}{\lVert \boldsymbol{w} \rVert}$ ，也就等价于最小化 $ \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2 $ （ $\frac{1}{2}$ 是为了后面求导以后形式简洁，不影响结果），因此SVM模型的求解最大分割超平面问题又可以表示为以下约束最优化问题

$$
 \underset{\boldsymbol{w,}b}{\min}\ \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2 
$$

$$
 s.t.\ \ y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \ge 1,\ i=1,2,...,N 
$$

这是一个含有不等式约束的凸二次规划问题，可以对其使用拉格朗日乘子法得到其对偶问题（dual problem）。

首先，我们将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数

$$
L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =\frac{1}{2}\lVert \boldsymbol{w} \rVert ^2-\sum_{i=1}^N{\alpha _i\left( y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1 \right)} 
$$

其中 $ \alpha _i $ 为拉格朗日乘子，且 $ \alpha _i\ge 0 $ 。现在我们令

$$
 \theta \left( \boldsymbol{w} \right) =\underset{\alpha _{_i}\ge 0}{\max}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) 
$$

当样本点不满足约束条件时，即在可行解区域外：

$$
 y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) <1 
$$

此时，将 $\alpha _i $ 设置为无穷大，则 $ \theta \left( \boldsymbol{w} \right) $ 也为无穷大。

当满本点满足约束条件时，即在可行解区域内：

$$
y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \ge 1 
$$

此时， $ \theta \left( \boldsymbol{w} \right) $ 为原函数本身。于是，将两种情况合并起来就可以得到我们新的目标函数

$$
 \theta \left( \boldsymbol{w} \right) =\begin{cases} \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2\ ,\boldsymbol{x}\in \text{可行区域}\\ +\infty \ \ \ \ \ ,\boldsymbol{x}\in \text{不可行区域}\\ \end{cases} 
$$

于是原约束问题就等价于

$$
 \underset{\boldsymbol{w,}b}{\min}\ \theta \left( \boldsymbol{w} \right) =\underset{\boldsymbol{w,}b}{\min}\underset{\alpha _i\ge 0}{\max}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =p^* 
$$

看一下我们的新目标函数，先求最大值，再求最小值。这样的话，我们首先就要面对带有需要求解的参数 $ \boldsymbol{w} $ 和 $ b$ 的方程，而 $ \alpha _i $ 又是不等式约束，这个求解过程不好做。所以，我们需要使用拉格朗日函数**对偶性**，将最小和最大的位置交换一下，这样就变成了：

$$
 \underset{\alpha _i\ge 0}{\max}\underset{\boldsymbol{w,}b}{\min}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =d^* 
$$

要有 $ p^*=d^* $ ，需要满足两个条件：

① 优化问题是凸优化问题

② 满足KKT条件

首先，本优化问题显然是一个凸优化问题，所以条件一满足，而要满足条件二，即要求

$$
 \begin{cases} \alpha _i\ge 0\\ y_i\left( \boldsymbol{w}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1\ge 0\\ \alpha _i\left( y_i\left( \boldsymbol{w}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1 \right) =0\\ \end{cases} 
$$

为了得到求解对偶问题的具体形式，令 $ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) $ 对 $ \boldsymbol{w}$ 和 $ b $ 的偏导为0，可得

$$
\boldsymbol{w}=\sum_{i=1}^N{\alpha _iy_i\boldsymbol{x}_{\boldsymbol{i}}} 
$$

$$
\sum_{i=1}^N{\alpha _iy_i}=0 
$$

将以上两个等式带入拉格朗日目标函数，消去 $ \boldsymbol{w}$ 和 $ b $ ， 得

$$
 L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}-\sum_{i=1}^N{\alpha _iy_i\left( \left( \sum_{j=1}^N{\alpha _jy_j\boldsymbol{x}_{\boldsymbol{j}}} \right) \cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) +}\sum_{i=1}^N{\alpha _i} 
$$

$$
\ \ \ \ \ \ \ \ \ \ \ =-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}+\sum_{i=1}^N{\alpha _i} 
$$

即 $\underset{\boldsymbol{w,}b}{\min}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}+\sum_{i=1}^N{\alpha _i} $

求 $ \underset{\boldsymbol{w,}b}{\min}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) $ 对 $ \boldsymbol{\alpha } $ 的极大，即是对偶问题

$$
\underset{\boldsymbol{\alpha }}{\max}\ -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}+\sum_{i=1}^N{\alpha _i} 
$$

$$
 s.t.\ \ \ \ \sum_{i=1}^N{\alpha _iy_i}=0 
$$

$$
 \ \ \ \ \ \ \ \alpha _i\ge 0,\ i=1,2,...,N
$$

把目标式子加一个负号，将求解极大转换为求解极小

$$
 \underset{\boldsymbol{\alpha }}{\min}\ \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}-\sum_{i=1}^N{\alpha _i} 
$$

$$
 s.t.\ \ \ \ \sum_{i=1}^N{\alpha _iy_i}=0 
$$

$$
\ \ \ \ \ \ \ \alpha _i\ge 0,\ i=1,2,...,N 
$$

现在我们的优化问题变成了如上的形式。对于这个问题，我们有更高效的优化算法，即序列最小优化（SMO）算法。这里暂时不展开关于使用SMO算法求解以上优化问题的细节，下一篇文章再加以详细推导。

我们通过这个优化算法能得到 $\boldsymbol{\alpha }^* $ ，再根据 $\boldsymbol{\alpha }^* $ ，我们就可以求解出 $ \boldsymbol{w}$ 和 $ b $ ，进而求得我们最初的目的：找到超平面，即”决策平面”。

前面的推导都是假设满足KKT条件下成立的，KKT条件如下

$$
 \begin{cases} \alpha _i\ge 0\\ y_i\left( \boldsymbol{w}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1\ge 0\\ \alpha _i\left( y_i\left( \boldsymbol{w}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1 \right) =0\\ \end{cases} 
$$

另外，根据前面的推导，还有下面两个式子成立

$$
\boldsymbol{w}=\sum_{i=1}^N{\alpha _iy_i\boldsymbol{x}_{\boldsymbol{i}}} 
$$

$$
\sum_{i=1}^N{\alpha _iy_i}=0 
$$

由此可知在 $\boldsymbol{\alpha }^* $ 中，至少存在一个 $ \alpha _{j}^{*}>0$ （反证法可以证明，若全为0，则 $ \boldsymbol{w}=0 $ ，矛盾），对此 $ j $ 有

$$
 y_j\left( \boldsymbol{w}^*\cdot \boldsymbol{x}_{\boldsymbol{j}}+b^* \right) -1=0 
$$

因此可以得到

$$
\boldsymbol{w}^*=\sum_{i=1}^N{\alpha _{i}^{*}y_i\boldsymbol{x}_i} 
$$

$$
 b^*=y_j-\sum_{i=1}^N{\alpha _{i}^{*}y_i\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)} 
$$

对于任意训练样本 $ \left( \boldsymbol{x}_{\boldsymbol{i}},y_i \right) $ ，总有 $ \alpha _i=0 $ 或者 $y_{j}\left(\boldsymbol{w} \cdot \boldsymbol{x}_{j}+b\right)=1$ 。若 $ \alpha _i=0 $ ，则该样本不会在最后求解模型参数的式子中出现。若 $ \alpha _i>0$ ，则必有 $ y_j\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{j}}+b \right) =1 $ ，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：**训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。**

到这里都是基于训练集数据线性可分的假设下进行的，但是实际情况下几乎不存在完全线性可分的数据，为了解决这个问题，引入了“软间隔”的概念，即允许某些点不满足约束

$$
y_j\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{j}}+b \right) \ge 1
$$

采用hinge损失，将原优化问题改写为

$$
\underset{\boldsymbol{w,}b,\xi _i}{\min}\ \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2+C\sum_{i=1}^m{\xi _i} 
$$

$$
 s.t.\ \ y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \ge 1-\xi _i 
$$

$$
 \ \ \ \ \ \xi _i\ge 0\ ,\ i=1,2,...,N 
$$

其中 $ \xi _i $ 为“松弛变量”， $\xi _i=\max \left( 0,1-y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \right) $ ，即一个hinge损失函数。每一个样本都有一个对应的松弛变量，表征该样本不满足约束的程度。 $ C>0 $ 称为惩罚参数， $C$ 值越大，对分类的惩罚越大。跟线性可分求解的思路一致，同样这里先用拉格朗日乘子法得到拉格朗日函数，再求其对偶问题。

综合以上讨论，我们可以得到**线性支持向量机学习算法**如下：

**输入**：训练数据集 $ T=\left\{ \left( \boldsymbol{x}_1,y_1 \right) ,\left( \boldsymbol{x}_1,y_1 \right) ,...,\left( \boldsymbol{x}_N,y_N \right) \right\}$ 其中， $ \boldsymbol{x}_i\in \mathbb{R}^n $ ， $ y_i\in \left\{ +1,-1 \right\} ,i=1,2,...N$ ；

**输出**：分离超平面和分类决策函数

（1）选择惩罚参数 $ C>0 $ ，构造并求解凸二次规划问题

$$
 \underset{\boldsymbol{\alpha }}{\min}\ \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}-\sum_{i=1}^N{\alpha _i}
$$

$$
 s.t.\ \ \ \ \sum_{i=1}^N{\alpha _iy_i}=0
$$

$$
 \ \ \ \ \ \ \ 0\le \alpha _i\le C,\ i=1,2,...,N
$$

得到最优解 $ \boldsymbol{\alpha }^*=\left( \alpha _{1}^{*},\alpha _{2}^{*},...,\alpha _{N}^{*} \right) ^T $

（2）计算

$$
 \boldsymbol{w}^*=\sum_{i=1}^N{\alpha _{i}^{*}y_i\boldsymbol{x}_i} 
$$

选择 $\boldsymbol{\alpha }^* $ 的一个分量 $ \alpha _{j}^{*} $ 满足条件 $ 0<\alpha _{j}^{*} ，计算

$$
b^*=y_j-\sum_{i=1}^N{\alpha _{i}^{*}y_i\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}
$$

（3）求分离超平面

$$
 \boldsymbol{w}^*\cdot \boldsymbol{x}+b^*=0 
$$

分类决策函数：

$$
 f\left( \boldsymbol{x} \right) =sign\left( \boldsymbol{w}^*\cdot \boldsymbol{x}+b^* \right) 
$$


## 5.用SVM（Python或Matlab开源库均可）对UCI网站的Iris数据进行分类。输出分类精度。测试集和训练集比例自行拟定。
见 iris_svm.ipynb


- [知乎-什么是支持向量？](https://zhuanlan.zhihu.com/p/106265377)
- [CSDN-输入空间、输出空间、特征空间与假设空间](https://blog.csdn.net/qq_41867448/article/details/84984676)
- [知乎-支持向量机（SVM）——原理篇](https://zhuanlan.zhihu.com/p/31886934)
- [博客园-Python中的支持向量机SVM的使用（有实例）](https://www.cnblogs.com/luyaoblog/p/6775342.html)
