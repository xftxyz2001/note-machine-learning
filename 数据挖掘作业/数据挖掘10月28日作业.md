## 1.监督学习和无监督学习的本质区别。
1. 有监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。

2. 有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，预先没有什么标签。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。

3. 非监督学习方法在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。这一点是比有监督学习方法的用途要广。譬如分析一堆数据的主分量，或分析数据集有什么特点都可以归于非监督学习方法的范畴。

4. 用非监督学习方法分析数据集的主分量与用K-L变换计算数据集的主分量又有区别。后者从方法上讲不是学习方法。因此用K-L变换找主分量不属于无监督学习方法，即方法上不是。而通过学习逐渐找到规律性这体现了学习方法这一点。在人工神经元网络中寻找主分量的方法属于无监督学习方法。


## 2.用k-means算法对Iris数据进行聚类，并计算聚类的准确率，每一聚类的标签为该聚类内占比最大的类别，其余类别按聚类错误处理。
见 iris_kmeans.ipynb


## 3.层次聚类中用来衡量类间距离的方法有哪些？

### 一.最近距离法
$$
D_{kl}=min\left[ d_{ij}\right]，式中d_{ij}表示x_{i}\in\omega_{k}和x_{j}\in\omega_{l}之间的距离,\omega_{k}和\omega_{l}分别表示两个类别
$$

### 二.最远距离法
$$
D_{kl}=max\left[ d_{ij}\right]，式中d_{ij}表示x_{i}\in\omega_{k}和x_{j}\in\omega_{l}之间的距离,\omega_{k}和\omega_{l}分别表示两个类别
$$

### 三.中间距离法
利用中心定理，求出类内的中心，然后找出不同类内中心距离的最小值

### 四.重心距离法
重心距离法和中心距离法类似，在某些情况下可以相同对待

### 五.平均距离法
$$
D_{kl}^{2}=\frac{1}{n_{k}n_{l}}\sum_{x_{i}\in\omega_{k},x_{j}\in\omega_{l}}^{b}{d_{ij}^{2}}\式中d_{ij}表示x_{i}\in\omega_{k}和x_{j}\in\omega_{l}之间的距离\\omega_{k}和\omega_{l}分别表示两个类别\ n_{k}和n_{l}分别表示k类和l类的样本点数
$$

对每两个类间不同点的距离的平方求和取平均


## 4.数据标准化有何作用？介绍了哪几类数据标准化方法？
数据标准化的作用：
- 有利于初始化的进行
- 避免给梯度数值的更新带来数值问题
- 有利于学习率数值的调整
- 搜索轨迹:加快寻找最优解速度

常见的方法有：min-max标准化（min-max normalization）、log函数转换、atan函数转换、z-score标准化（zero-mena normalization，此方法比较常用）、模糊量化法。

**1.min-max标准化（归一化）**

也叫极差标准化法，是消除变量量纲和变异范围影响最简单的方法。

具体方法：找出每个属性的最小值和最大值，将其一个原始值x通过min-max标准化映射成在区间\[0,1\]中的值x'。

公式：X' = (X-Xmin) / (Xmax-Xmin)

无论原始数据是正值还是负值，经过处理后，该变量各个观察值的数值变化范围都满足0≤X'≤1，并且正指标、逆指标均可转化为正向指标，作用方向一致。

但如果有新数据加入，就可能会导致最大值（Xmax）和最小值（Xmin）发生变化，就需要进行重新定义，并重新计算极差（R）。


**2.z-score标准化（规范化）**

当遇到某个指标的最大值和最小值未知的情况，或有超出取值范围的离群数值时，上面的方法就不再适用了，可以采用另一种数据标准化最常用的方法，即**Z-score标准化，也叫标准差标准化法**。

它基于原始数据的均值（mean）和标准差（standarddeviation）进行数据的标准化，将A的原始值x使用z-score标准化到x'。

z-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。

新数据=（原数据- 均值）/ 标准差

均值和标准差都是在样本集上定义的，而不是在单个样本上定义的。标准化是针对某个属性的，需要用到所有样本在该属性上的值。

数据标准化**最常用**的是这两种：**极差标准化法**、**Z-score标准化法**。


**3.线性比例标准化法**

（1）极大化法

对于正指标，取该指标的最大值Xmax，然后用该变量的每一个观察值除以最大值，即：X' =X / Xmax。（X≥0）

（2）极小化法

对于逆指标，取该指标的最小值Xmin，然后用该变量的最小值除以每一个观察值，即：X' = Xmin / X。（X＞0）

注：以上两种方法不适用于X<0的情况。

**正指标**：是一种评估指标，指标数值大小与司法绩效高低一致；正指标越大越好。

**逆指标**：是强度相对的指标，指标数值大小与司法绩效高低相反；逆指标越小越好。

对于**逆向指标**使用线性比例法进行标准化后，实际上是进行了非线性的变换，变换后的指标无法客观地反映原始指标的相互关系，转换时需要注意。


**4.log函数标准化法**

首先对该变量的每一个观察值取以10为底的log值，然后再除以该指标最大值（Xmax）的log值，即：

X' = log10(X) / log10 (Xmax)

注：x=log10(X)的区间不一定在\[0,1\]；Xmax为样本最大值；此方法要求X≥1。


**5.反正切函数标准化法**

通过三角函数中的反正切函数（arctan）也可以实现数据的标准化转换。

计算方法：X' = arctan(X)\*2 / π

注：如果原始数据为正、负实数，则标准化后的数据区间为-1≤X'≤1；若要得到0≤X'≤1区间，则原始数据应该保证X≥0。


**6.atan函数转换**

用反正切函数也可以实现数据的归一化，使用这个方法需要注意的是如果想映射的区间为\[0,1\]，则**数据都应该大于等于0，小于0的数据将被映射到\[-1,0\]区间上。**


**7.归一化**

把数据映射到0-1范围内，使数据处理更加便捷。

Xn=Xi/(X1+X2+……+Xn);Xn的和=1。


## 5.聚类的评估方法有哪些？
- 外部评估方法：在知道真实标签（ground truth）的情况下来评估聚类结果的好坏。是用有监督的数据去评测无监督训练的结果。本文不讨论这个，请参考：空字符：几种常见的聚类评价指标。

- 内部评估法：不借助于外部信息，仅仅只是根据聚类结果来进行评估。
  - 只考虑簇内相似度
    - 簇内误差平方和 （within-cluster sum of square error, 简称SSE）
    - 紧密性 （Compactness， 简称CP）
  - 只考虑簇间的情况
    - 间隔性 （Separation，简称SP）
  - 簇间都考虑
    - 轮廓系数 （Silhouette Coefficient）
    - CH指数（Calinski-Harabaz Index）
    - 戴维森堡丁指数（Davies-Bouldin Index，简称DBI）
    - 邓恩指数（Dunn Validity Index，简称DVI）


## 6.聚类的表示方法有哪几类？

### 1. K-Means聚类
算法步骤：  
1. 首先我们选择一些类/组，并随机初始化它们各自的中心点。中心点是与每个数据点向量长度相同的位置。这需要我们提前预知类的数量(即中心点的数量)。  
2. 计算每个数据点到中心点的距离，数据点距离哪个中心点最近就划分到哪一类中。  
3. 计算每一类中中心点作为新的中心点。  
4. 重复以上步骤，直到每一类中心在每次迭代后变化不大为止。也可以多次随机初始化中心点，然后选择运行结果最好的一个。  

下图演示了K-Means进行分类的过程：  
![这里写图片描述](https://img-blog.csdn.net/20180228115245278)  
优点：  
速度快，计算简便  
缺点：  
我们必须提前知道数据有多少类/组。  
K-Medians是K-Means的一种变体，是用数据集的中位数而不是均值来计算数据的中心点。  
K-Medians的优势是使用中位数来计算中心点不受异常值的影响；缺点是计算中位数时需要对数据集中的数据进行排序，速度相对于K-Means较慢。

### 2. 均值漂移聚类
均值漂移聚类是基于滑动窗口的算法，来找到数据点的密集区域。这是一个基于质心的算法，通过将中心点的候选点更新为滑动窗口内点的均值来完成，来定位每个组/类的中心点。然后对这些候选窗口进行相似窗口进行去除，最终形成中心点集及相应的分组。  
具体步骤：  
1. 确定滑动窗口半径r，以随机选取的中心点C半径为r的圆形滑动窗口开始滑动。均值漂移类似一种爬山算法，在每一次迭代中向密度更高的区域移动，直到收敛。  
2. 每一次滑动到新的区域，计算滑动窗口内的均值来作为中心点，滑动窗口内的点的数量为窗口内的密度。在每一次移动中，窗口会想密度更高的区域移动。  
3. 移动窗口，计算窗口内的中心点以及窗口内的密度，知道没有方向在窗口内可以容纳更多的点，即一直移动到圆内密度不再增加为止。  
4. 步骤一到三会产生很多个滑动窗口，当多个滑动窗口重叠时，保留包含最多点的窗口，然后根据数据点所在的滑动窗口进行聚类。  

下图演示了均值漂移聚类的计算步骤：  
![这里写图片描述](https://img-blog.csdn.net/2018022816141991)  
下面显示了所有滑动窗口从头到尾的整个过程。每个黑点代表滑动窗口的质心，每个灰点代表一个数据点。  
![这里写图片描述](https://img-blog.csdn.net/20180228161543)  
优点：（1）不同于K-Means算法，均值漂移聚类算法不需要我们知道有多少类/组。  
（2）基于密度的算法相比于K-Means受均值影响较小。  
缺点：（1）窗口半径r的选择可能是不重要的。

### 3. 基于密度的聚类方法(DBSCAN)
与均值漂移聚类类似，DBSCAN也是基于密度的聚类算法。  
具体步骤：  
1. 首先确定半径r和minPoints. 从一个没有被访问过的任意数据点开始，以这个点为中心，r为半径的圆内包含的点的数量是否大于或等于minPoints，如果大于或等于minPoints则改点被标记为central point,反之则会被标记为noise point。  
2. 重复1的步骤，如果一个noise point存在于某个central point为半径的圆内，则这个点被标记为边缘点，反之仍为noise point。重复步骤1，知道所有的点都被访问过。  
优点：不需要知道簇的数量  
缺点：需要确定距离r和minPoints

### 4. 用高斯混合模型（GMM）的最大期望（EM）聚类
K-Means的缺点在于对聚类中心均值的简单使用。下面的图中的两个圆如果使用K-Means则不能作出正确的类的判断。同样的，如果数据集中的点类似下图中曲线的情况也是不能正确分类的。  
![这里写图片描述](https://img-blog.csdn.net/20180228114540606)  
使用高斯混合模型（GMM）做聚类首先假设数据点是呈高斯分布的，相对应K-Means假设数据点是圆形的，高斯分布（椭圆形）给出了更多的可能性。我们有两个参数来描述簇的形状：均值和标准差。所以这些簇可以采取任何形状的椭圆形，因为在x，y方向上都有标准差。因此，每个高斯分布被分配给单个簇。  
所以要做聚类首先应该找到数据集的均值和标准差，我们将采用一个叫做最大期望(EM)的优化算法。下图演示了使用GMMs进行最大期望的聚类过程。  
![这里写图片描述](https://img-blog.csdn.net/20180301110131231)  
具体步骤：  
1. 选择簇的数量（与K-Means类似）并随机初始化每个簇的高斯分布参数（均值和方差）。也可以先观察数据给出一个相对精确的均值和方差。  
2. 给定每个簇的高斯分布，计算每个数据点属于每个簇的概率。一个点越靠近高斯分布的中心就越可能属于该簇。  
3. 基于这些概率我们计算高斯分布参数使得数据点的概率最大化，可以使用数据点概率的加权来计算这些新的参数，权重就是数据点属于该簇的概率。  
4. 重复迭代2和3直到在迭代中的变化不大。  
GMMs的优点：（1）GMMs使用均值和标准差，簇可以呈现出椭圆形而不是仅仅限制于圆形。K-Means是GMMs的一个特殊情况，是方差在所有维度上都接近于0时簇就会呈现出圆形。  
（2）GMMs是使用概率，所有一个数据点可以属于多个簇。例如数据点X可以有百分之20的概率属于A簇，百分之80的概率属于B簇。也就是说GMMs可以支持混合资格。

### 5. 凝聚层次聚类
层次聚类算法分为两类：自上而下和自下而上。凝聚层级聚类(HAC)是自下而上的一种聚类算法。HAC首先将每个数据点视为一个单一的簇，然后计算所有簇之间的距离来合并簇，知道所有的簇聚合成为一个簇为止。  
下图为凝聚层级聚类的一个实例：  
![这里写图片描述](https://img-blog.csdn.net/20180301171047257)  
具体步骤：  
1. 首先我们将每个数据点视为一个单一的簇，然后选择一个测量两个簇之间距离的度量标准。例如我们使用average linkage作为标准，它将两个簇之间的距离定义为第一个簇中的数据点与第二个簇中的数据点之间的平均距离。  
2. 在每次迭代中，我们将两个具有最小average linkage的簇合并成为一个簇。  
3. 重复步骤2知道所有的数据点合并成一个簇，然后选择我们需要多少个簇。  
层次聚类优点：（1）不需要知道有多少个簇  
（2）对于距离度量标准的选择并不敏感  
缺点：效率低

### 6. 图团体检测(Graph Community Detection)
当我们的数据可以被表示为网络或图是，可以使用图团体检测方法完成聚类。在这个算法中图团体（graph community）通常被定义为一种顶点(vertice)的子集，其中的顶点相对于网络的其他部分要连接的更加紧密。下图展示了一个简单的图，展示了最近浏览过的8个网站，根据他们的维基百科页面中的链接进行了连接。  
![这里写图片描述](https://img-blog.csdn.net/20180301163648779)  
模块性可以使用以下公式进行计算：  
M=12L∑Ni,j=1(Aij−kiKj2L)δCi,CjM=12L∑i,j=1N(Aij−kiKj2L)δCi,CjM=frac{1}{2L}sum_{i,j=1}^{N}(A_{ij}-frac{k_iK_j}{2L})delta_{C_{i},C{j}}  
其中L代表网络中边的数量，AijAijA_{ij}代表真实的顶点i和j之间的边数, ki,kjki,kjk_i, k_j代表每个顶点的degree，可以通过将每一行每一列的项相加起来而得到。两者相乘再除以2L表示该网络是随机分配的时候顶点i和j之间的预期边数。所以Aij−kikj2LAij−kikj2LA_{ij}-frac{k_ik_j}{2L}代表了该网络的真实结构和随机组合时的预期结构之间的差。当AijAijA_{ij}为1时，且kikj2Lkikj2Lfrac{k_ik_j}{2L}很小的时候，其返回值最高。也就是说，当在定点i和j之间存在一个非预期边是得到的值更高。  
δCi,CjδCi,Cjdelta_{C_i, C_j}是克罗内克δδ\delta函数(Kronecker-delta function). 下面是其Python解释：

```python
def Kronecker_Delta(ci,cj):
    if ci==cj:
        return 1
    else:
        return 0
```

通过上述公式可以计算图的模块性，且模块性越高，该网络聚类成不同团体的程度越好，因此通过最优化方法寻找最大模块性就能发现聚类该网络的最佳方法。  
组合学告诉我们对于一个仅有8个顶点的网络，就存在4140种不同的聚类方式，16个顶点的网络的聚类方式将超过100亿种。32个顶点的网络的可能聚类方式更是将超过10^21种。因此，我们必须寻找一种启发式的方法使其不需要尝试每一种可能性。这种方法叫做Fast-Greedy Modularity-Maximization(快速贪婪模块性最大化)的算法，这种算法在一定程度上类似于上面描述的集聚层次聚类算法。只是这种算法不根据距离来融合团体，而是根据模块性的改变来对团体进行融合。  
具体步骤：  
1. 首先初始分配每个顶点到其自己的团体，然后计算整个网络的模块性 M。  
2. 第 1 步要求每个团体对（community pair）至少被一条单边链接，如果有两个团体融合到了一起，该算法就计算由此造成的模块性改变 ΔM。  
3. 第 2 步是取 ΔM 出现了最大增长的团体对，然后融合。然后为这个聚类计算新的模块性 M，并记录下来。  
4. 重复第 1 步和 第 2 步——每一次都融合团体对，这样最后得到 ΔM 的最大增益，然后记录新的聚类模式及其相应的模块性分数 M。  
5. 重复第 1 步和 第 2 步——每一次都融合团体对，这样最后得到 ΔM 的最大增益，然后记录新的聚类模式及其相应的模块性分数 M。


## 7.介绍了哪些距离函数？
1. 欧氏距离
2. 曼哈顿距离
3. 切比雪夫距离
4. 闵可夫斯基距离
5. 标准化欧氏距离
6. 马氏距离
7. 夹角余弦
8. 汉明距离
9. 杰卡德距离 & 杰卡德相似系数
10. 相关系数 & 相关距离
11. 信息熵


## 8.混合属性的处理方法有哪几种？
混合类型相异度计算的思想：按不同类型的属性（如数值型，二元变量，名义变量等），根据各自类型的计算方法计算之后再加权求和。

可以用加权法计算合并的影响
- 如果属性  f  是二进制 (二元 ) 属性，则  $\mathrm{d}_{\mathrm{ij}}(\mathrm{f})=0  if  \mathrm{x}_{\mathrm{if}}=\mathrm{x}_{\mathrm{jf}}  ， or  \mathrm{d}_{\mathrm{ij}}(\mathrm{f})=1  otherwise$
- 如果属性f是标称属性，则可以化为一系列二元属性
- 如果属性  f  数值型，则规范化后可以通过距离计算其相异性
- 如果属性  f  序数属性，则可以通过前面的方法计算其相异性
- 最后可以计算其相异度的加权平均值作为混合型属性的邻近性度量

$d(i, j)=\frac{\sum_{f=1}^{p} \delta_{i j}^{(f)} d_{i j}^{(f)}}{\sum_{f=1}^{p} \delta_{i j}^{(f)}}$


- [CSDN-有监督学习与无监督学习的区别](https://blog.csdn.net/zhao2chen3/article/details/115164508)
- [CSDN-使用KMeans对iris数据集聚类](https://blog.csdn.net/weixin_50197893/article/details/122464886)
- [知乎-聚类分析的类间距离度量方法](https://zhuanlan.zhihu.com/p/106527475)
- [CSDN-神经网络为什么要归一化](https://blog.csdn.net/fontthrone/article/details/74064971)
- [知乎-5.数据预处理—数据标准化（三）](https://zhuanlan.zhihu.com/p/343692147)
- [知乎-聚类的评测方法](https://zhuanlan.zhihu.com/p/436376880)
- [CSDN-常见的六大聚类算法](https://blog.csdn.net/Katherine_hsr/article/details/79382249)
- [CSDN-各种距离函数](https://blog.csdn.net/haoji007/article/details/81157450)
