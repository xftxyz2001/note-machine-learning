## 1.解释在软边距支持向量机(Soft-marginSVM)中松弛变量xi（小写）和惩罚因子C的作用。
对于一个分类问题，其样本集中往往存在一些模糊不清的样本，直观上，考虑SVM分类器，这些样本往往处于超平面的附近。这些样本对于分类器的设计具有很大的影响。一种常用的方法为引入松弛因子到约束条件中，同时需要将这种松弛所导致的惩罚因子加入到目标函数中。

![](http://upload-images.jianshu.io/upload_images/5521533-8311b6e0e5fdfb23.png?imageMogr2/auto-orient/strip|imageView2/2/w/471/format/webp)

引入松弛因子和惩罚函数后的优化模型

实际模型中，我们并不关心松弛因子，其数值是根据样本点到假设超平面的距离给出的，所以，我们只需要设定惩罚因子C的数值即可。C的值可以为固定的一个值，也可以是一个向量。这里可以引申出分类中**不均衡**的问题，即，样本中正分类和负分类的数量具有较大差异。这时我们可以通过引入两种惩罚因子C，人为的增加对较少样本数量类别的重视程度，进而改善不均衡问题。

![](http://upload-images.jianshu.io/upload_images/5521533-92248f5384435569.png?imageMogr2/auto-orient/strip|imageView2/2/w/279/format/webp)


## 2.什么是特征空间和输入空间。
输入空间：输入所有可能的取值集合。可以是有限个元素的集合，也可以是整个欧式空间（欧几里得发明的一个特别的度量空间）。
每个具体的输入是一个实例，通常由特征向量表示。所有的特征向量存在的空间成为特征空间。特征空间的每一维对应于一个特征。

特征向量：$A*\alpha =\lambda *\alpha$

其中，A是一个方阵，也就是行数和列数相同的矩阵；是一个不为零的向量，一般是列向量；是实数。

注：有时候假设输入空间和特征空间是相同的空间，对它们不予区分；但有时候也假设他们是不同的空间，把实例从输入空间映射到特征空间。模型实际上都是定义在特征空间的。

输出空间：输出所有可能的集合。与输入空间对应。同样可以是有限个元素的集合，也可以是整个欧式空间（欧几里得发明的一个特别的度量空间）。

假设空间：输入空间到输出空间映射的集合。这一映射由模型来表示，换句话说，这是学习模型的集合。


## 3.核函数的作用？
核函数，统计学术语，支持向量机通过某非线性变换 φ(x)，将输入空间映射到高维特征空间。特征空间的维数可能非常高。
如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数 K(x, x′)，它恰好等于在高维空间中这个内积，即K( x, x′) =<φ( x) ⋅φ( x′) > 。
那么支持向量机就不用计算复杂的非线性变换，而由这个函数 K(x, x′) 直接得到非线性变换的内积，使大大简化了计算。这样的函数 K(x, x′) 称为核函数。
核函数只是用来计算映射到高维空间之后的内积的一种简便方法。
核函数的作用就是隐含着一个从低维空间到高维空间的映射，而这个映射可以把低维空间中线性不可分的两类点变成线性可分的。
事实中使用的核函数往往复杂得多。它们对应的映射并不一定能够显式地表达出来；它们映射到的高维空间的维数也高得多，甚至是无穷维的。这样，就可以期待原来并不线性可分的两类点变成线性可分的了。


## 4.用k-近邻学习方法对Iris数据进行分类，测试数据比例不限。试验若干个不同的k值。
见 iris_knn.ipynb
```python
# %%
# 导入sklearn算法包
from sklearn.linear_model import LogisticRegression  # 逻辑回归
from sklearn.naive_bayes import GaussianNB  # 朴素贝叶斯
from sklearn.neighbors import KNeighborsClassifier  # K-近邻
from sklearn.tree import DecisionTreeClassifier  # 决策树
from sklearn import svm  # 支持向量机


# %%
# 使用转换函数映射成浮点类型的数据
def iris_type(s):
    it = {b'Iris-setosa': 0, b'Iris-versicolor': 1, b'Iris-virginica': 2}
    return it[s]


# %%
# 使用numpy中的loadtxt读入数据文件
import numpy as np
path = u'./iris/iris.data'  # 数据文件路径
data = np.loadtxt(path, dtype=float, delimiter=',', converters={4: iris_type})


# %%
# 将Iris分为训练集与测试集
from sklearn.model_selection import train_test_split

x, y = np.split(data, (4,), axis=1)
x = x[:, :2]
x_train, x_test, y_train, y_test = train_test_split(
    x, y, random_state=1, train_size=0.6)


# %%
# 训练svm分类器
# clf = svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr')
clf = svm.SVC(C=0.8, kernel='rbf', gamma=20, decision_function_shape='ovr')
clf.fit(x_train, y_train.ravel())


# %%
# 计算svc分类器的准确率
# show_accuracy
from sklearn.metrics import accuracy_score
print(clf.score(x_train, y_train))  # 精度
y_hat = clf.predict(x_train)
accuracy_score(y_train, y_hat)  # , '训练集')
print(clf.score(x_test, y_test))
y_hat = clf.predict(x_test)
accuracy_score(y_test, y_hat)  # , '测试集')


# %%
# 查看决策函数
print('decision_function:\n', clf.decision_function(x_train))
print('\npredict:\n', clf.predict(x_train))


# %%
# 绘制图像
import matplotlib as mpl
import matplotlib.pyplot as plt

x1_min, x1_max = x[:, 0].min(), x[:, 0].max()  # 第0列的范围
x2_min, x2_max = x[:, 1].min(), x[:, 1].max()  # 第1列的范围
x1, x2 = np.mgrid[x1_min:x1_max:200j, x2_min:x2_max:200j]  # 生成网格采样点
grid_test = np.stack((x1.flat, x2.flat), axis=1)  # 测试点
# print 'grid_test = \n', grid_test
grid_hat = clf.predict(grid_test)       # 预测分类值
grid_hat = grid_hat.reshape(x1.shape)  # 使之与输入的形状相同

# 指定默认字体
mpl.rcParams['font.sans-serif'] = [u'SimHei']
mpl.rcParams['axes.unicode_minus'] = False


# %%
cm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])
cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])
plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light)
plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k', s=50, cmap=cm_dark)  # 样本
plt.scatter(x_test[:, 0], x_test[:, 1], s=120,
            facecolors='none', zorder=10)  # 圈中测试集样本
plt.xlabel(u'花萼长度', fontsize=13)
plt.ylabel(u'花萼宽度', fontsize=13)
plt.xlim(x1_min, x1_max)
plt.ylim(x2_min, x2_max)
plt.title(u'鸢尾花SVM二特征分类', fontsize=15)
# plt.grid()
plt.show()
```

## 5.简述Bagging和Boosting的思想或流程。

### 一.boosting

#### 1.思想
其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

#### 2.代表模型及其原理
AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。  
而GBDT-梯度提升决策树树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

### 二.bagging(bootstrap aggregating)

#### 1.思想
Bagging即套袋法，其思想是:  
- 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
- 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
- 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

#### 2.代表模型及其原理
随机森林(RF)  
1. 随机森林的弱学习器都是决策树。  
2. 随机森林在bagging的样本随机采样的基础上,又加上了特征的随机选择。


- [嘉士伯的Java小屋-SVM入门（八）松弛变量](http://www.blogjava.net/zhenandaci/archive/2009/03/15/259786.html)
- [嘉士伯的Java小屋-SVM入门（九）松弛变量（续）](http://www.blogjava.net/zhenandaci/archive/2009/03/17/260315.html)
- [简书-松弛因子和惩罚因子](https://www.jianshu.com/p/44dc71f1a222)
- [CSDN-输入空间、输出空间、特征空间与假设空间](https://blog.csdn.net/Lois_llw/article/details/123477402)
- [知乎-机器学习有很多关于核函数的说法，核函数的定义和作用是什么？](https://www.zhihu.com/question/24627666/answer/28440943)
- [CSDN-python 机器学习——K 近邻分类理论及鸢尾（ Iris ）数据集实例操作](https://blog.csdn.net/qq_39594033/article/details/107681835)
- [CSDN-bagging和boosting的思想简述各自的代表模型原理](https://blog.csdn.net/weixin_45834085/article/details/103073676)
