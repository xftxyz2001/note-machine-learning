## 1.决策树中处理连续属性值的方式。
1. 假设在样本集合D上特征 a 有 n 个取值，先把 n 个值从小到大排个序。
2. 确定一个阈值，把样本集合分成两部分。阈值怎么来呢，取排序后两个相邻的值的均值（那么n个值就有 n-1 个阈值），相当于n个小朋友从矮到高站成一排，你选个位置分成两堆，位置就等同于阈值
3. 分别比较这 n-1 个阈值的信息增益，选使得信息增益最大的那个值作为阈值来划分


## 2.过拟合的含义。
过拟合是指为了得到一致假设而使假设变得过度严格。避免过拟合是分类器设计中的一个核心任务。通常采用增大数据量和测试样本集的方法对分类器性能进行评价。

过拟合（over-fitting）也称为过学习，它的直观表现是算法在训练集上表现好，但在测试集上表现不好，泛化性能差。过拟合是在模型参数拟合过程中由于训练数据包含抽样误差，在训练时复杂的模型将抽样误差也进行了拟合导致的。所谓抽样误差，是指抽样得到的样本集和整体数据集之间的偏差。直观来看，引起过拟合的可能原因有：

模型本身过于复杂，以至于拟合了训练样本集中的噪声。此时需要选用更简单的模型，或者对模型进行裁剪。

训练样本太少或者缺乏代表性。此时需要增加样本数，或者增加样本的多样性。

训练样本噪声的干扰，导致模型拟合了这些噪声，这时需要剔除噪声数据或者改用对噪声不敏感的模型。


## 3.分类器的评估方法有哪些。
- 分类器的评估方法（ evaluations measures for classier ）
  - TP / TN / FP / FN
  - Precision / Recall / F-score
  - TPR / FPR / ACC
  - ROC curves (Receiver Operating Characteristic 接收器操作特性曲线)

- 回归模型的评估方法（ evaluations measures for regression ）
  - Root Mean Square Error (RMSE 均方根误差)
  - Cross Validation (交叉验证)


## 4.查准率和查全率的含义。
查准率
查准率（精度）是衡量某一检索系统的信号噪声比的一种指标，即检出的相关文献量与检出的文献总量的百分比。普遍表示为：查准率=（检索出的相关信息量/检索出的信息总量）x100%。使用专指性较强的检索语言(如上位类、上位主题词)能提高查准率，但查全率下降。

查全率
查全率（召回率），是衡量某一检索系统从文献集合中检出相关文献成功度的一项指标，即检出的相关文献量与检索系统中相关文献总量的百分比。普遍表示为：查全率=（检索出的相关信息量/系统中的相关信息总量）x100%。使用泛指性较强的检索语言(如上位类、上位主题词)能提高查全率，但查准率下降。


## 5.ROC曲线和提升曲线的含义。
ROC曲线
- 受试者工作特征曲线 （receiver operating characteristic curve，简称ROC曲线），又称为感受性曲线（sensitivity curve）。得此名的原因在于曲线上各点反映着相同的感受性，它们都是对同一信号刺激的反应，只不过是在几种不同的判定标准下所得的结果而已。接受者操作特性曲线就是以假阳性概率（False positive rate）为横轴，击中概率为纵轴所组成的坐标图，和被试在特定刺激条件下由于采用不同的判断标准得出的不同结果画出的曲线。
- ROC曲线是根据一系列不同的二分类方式（分界值或决定阈），以真阳性率（灵敏度）为纵坐标，假阳性率（1-特异度）为横坐标绘制的曲线。传统的诊断试验评价方法有一个共同的特点，必须将试验结果分为两类，再进行统计分析。ROC曲线的评价方法与传统的评价方法不同，无须此限制，而是根据实际情况，允许有中间状态，可以把试验结果划分为多个有序分类，如正常、大致正常、可疑、大致异常和异常五个等级再进行统计分析。因此，ROC曲线评价方法适用的范围更为广泛。

提升曲线
- 提升是预测模型有效性的度量，计算为使用和不使用预测模型获得的结果之间的比率。
- 累积增益和提升图是测量模型性能的可视化辅助工具。
- 两个图表都由提升曲线和基线组成。
- 提升曲线和基线之间的面积越大，模型越好。


## 6.朴素贝叶斯分类器中最重要的假设是？
朴素贝叶斯分类器(Naïve Bayes classifier)是一种相当简单常见但是又相当有效的分类算法
朴素贝叶斯分类器采用了“属性条件独立假设”（attribute conditional independent assumption），通俗来讲，就是一个属性，或者说是特征，相互之间是独立的；也正是有了这个假设，朴素贝叶斯分类器才能做这么多事情，在监督学习的世界里有着这么广泛的应用。

朴素贝叶斯听起来很简单，但是并不naive（朴素），而仅仅只是它做出了一个很naive（朴素）的假设，即属性条件独立假设，而这个假设在现实生活中并不一定成立，这就会引起偏差。针对这一点，有版朴素贝叶斯分类器将属性条件独立假设放松为独依赖假设，由此获得泛化性能的提升。


## 7.根据下图数据，用朴素贝叶斯分类方法预测 \<Outlook=sunny, Temperature=cool, Humidity=high, Wind=strong\> 的类标（写出计算过程）。
![](https://img.ketangpai.com/ketangpai.aliapp.com/1/webroot/Uploads/Download/2021-09-26/6150778299738.png)

| Day   | Outlook  | Temperature | Humidity | Wind   | Play Tennis |
| ----- | -------- | ----------- | -------- | ------ | ----------- |
| Day1  | Sunny    | Hot         | High     | Weak   | No          |
| Day2  | Sunny    | Hot         | High     | Strong | No          |
| Day3  | Overcast | Hot         | High     | Weak   | Yes         |
| Day4  | Rain     | Mild        | High     | Weak   | Yes         |
| Day5  | Rain     | Cool        | Normal   | Weak   | Yes         |
| Day6  | Rain     | Cool        | Normal   | Strong | No          |
| Day7  | Overcast | Cool        | Normal   | Strong | Yes         |
| Day8  | Sunny    | Mild        | High     | Weak   | No          |
| Day9  | Sunny    | Cool        | Normal   | Weak   | Yes         |
| Day10 | Rain     | Mild        | Normal   | Weak   | Yes         |
| Day11 | Sunny    | Mild        | Normal   | Strong | Yes         |
| Day12 | Overcast | Mild        | High     | Strong | Yes         |
| Day13 | Overcast | Hot         | Normal   | Weak   | Yes         |
| Day14 | Rain     | Mild        | High     | Strong | No          |

PlayData.csv
```
Sunny,Hot,High,Weak,1,No
Sunny,Hot,High,Strong,1,No
Overcast,Hot,High,Weak,1,Yes
Rain,Mild,High,Weak,1,Yes
Rain,Cool,Normal,Weak,1,Yes
Rain,Cool,Normal,Strong,1,No
Overcast,Cool,Normal,Strong,1,Yes
Sunny,Mild,High,Weak,1,No
Sunny,Cool,Normal,Weak,1,Yes
Rain,Mild,Normal,Weak,1,Yes
Sunny,Mild,Normal,Strong,1,Yes
Overcast,Mild,High,Strong,1,Yes
Overcast,Hot,Normal,Weak,1,Yes
Rain,Mild,High,Strong,1,No
```

```python
import random

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from collections import defaultdict
'''
朴素贝叶斯代码的实现步骤
    提取数据集数据
    分析处理数据集数据
    计算概率（先验概率、条件概率、联合概率）
    根据贝叶斯公式计算预测概率
其中处理不同数据类型（伯努利，多项式，连续型）和0概率情况，还用到了：
    概率密度函数
    拉普拉斯平滑
'''
# 定义属性值
outlook = ["Sunny", "Overcast", "Rain"]
Temperature = ["Hot", "Mild", "Cool"]
Humidity = ["High", "Normal"]
Wind = ["Strong", "Weak"]
PlayTennis = ["Yes", "No"]
Play = []
Play.append(outlook)
Play.append(Temperature)
Play.append(Humidity)
Play.append(Wind)
Play.append(PlayTennis)

# 数据集
data = [["Sunny", "Hot", "High", "Weak", "No"],
        ["Sunny", "Hot", "High", "Strong", "No"],
        ["Overcast", "Hot", "High", "Weak", "Yes"],
        ["Rain", "Mild", "High", "Weak", "Yes"],
        ["Rain", "Cool", "Normal", "Weak", "Yes"],
        ["Rain", "Cool", "Normal", "Strong", "No"],
        ["Overcast", "Cool", "Normal", "Strong", "Yes"],
        ["Sunny", "Mild", "High", "Weak", "No"],
        ["Sunny", "Cool", "Normal", "Weak", "Yes"],
        ["Rain", "Mild", "Normal", "Weak", "Yes"],
        ["Sunny", "Mild", "Normal", "Strong", "Yes"],
        ["Overcast", "Mild", "High", "Strong", "Yes"],
        ["Overcast", "Hot", "Normal", "Weak", "Yes"],
        ["Rain", "Mild", "High", "Strong", "No"],
        ]

length = len(data)
random.shuffle(data)
for i in range(length):
    print(data[i])
# train = data[:10]
train = data
train_length = len(train)
# test = data[10:]
# sunny,cool,high,strong
test = ["Sunny", "Cool", "High", "Strong"]
# test_length = len(test)


def count_PlayTennis_total(data):
    count = defaultdict(int)
    for i in range(train_length):
        count[data[i][4]] += 1
    return count

# 先验概率


def cal_base_rates(data):
    y = count_PlayTennis_total(data)
    cal_base_rates = {}
    for label in y.keys():
        priori_prob = (y[label]+1) / (len(train)+2)
        cal_base_rates[label] = priori_prob
    return cal_base_rates


print(cal_base_rates(train))


def count_sj(attr, Play):
    for i in range(len(Play)):
        if attr in Play[i]:
            return len(Play[i])

# 似然概率p(x|y) 也叫条件概率


def likelihold_prob(data):
    # 计算各个特征值在已知结果下的概率（likelihood probabilities）
    y = count_PlayTennis_total(data)
    likelihold = {}
    for i, c in y.items():
        # 创建一个临时的字典，临时存储各个特征值的概率
        attr_prob = defaultdict(int)
        for j in range(train_length):
            if data[j][4] == i:
                for attr in range(4):
                    attr_prob[data[j][attr]] += 1
        for keys, values in attr_prob.items():
            sj = count_sj(keys, Play)
            attr_prob[keys] = (values+1)/(c+sj)
        likelihold[i] = attr_prob
    return likelihold


LikeHold = likelihold_prob(train)
print(LikeHold)
############################################################


def Test(data, test):
    y = count_PlayTennis_total(data)
    likehold = likelihold_prob(data)
    playtennis = cal_base_rates(data)
    RATE = defaultdict(int)
    print(test)
    for i, _ in y.items():
        rates = 1
        for j in range(4):
            attr = test[j]
            rates *= likehold[i][attr]
        rates = rates * playtennis[i]
        RATE[i] = rates
    print("预测结果: ")
    print(RATE)
    return sorted(RATE, key=lambda x: RATE[x])[-1]


if __name__ == '__main__':
    print(cal_base_rates(train))
    print(likelihold_prob(train))
    # print(Test(train, test[0][:4]))
    # print(Test(train, test[1][:4]))
    # print(Test(train, test[2][:4]))
    # print(Test(train, test[3][:4]))
    print(Test(train, test))
```


```
sunny,cool,high,strong
```
预测结果:
```
defaultdict(<class 'int'>, {'Yes': 0.006887052341597796, 'No': 0.01913265306122449})
No
```


## 8.尝试编写贝叶斯分类器，用UCI网站上提供的Adult数据对分类器进行测试。可只用Adult数据集中的离散属性进行分类。
trees.py
```python
# -*- coding: utf-8 -*-
from math import log
import operator
 
def createDataSet():
    dataSet = [[1, 1, 'yes'],
               [1, 1, 'yes'],
               [1, 0, 'no'],
               [0, 1, 'no'],
               [0, 1, 'no']]
    labels = ['no surfacing','flippers']
    #change to discrete values
    return dataSet, labels
 
def calcShannonEnt(dataSet):   #计算给定数据集的香农熵
    numEntries = len(dataSet)   #实例总数
    labelCounts = {}
    for featVec in dataSet: #the the number of unique elements and their occurance
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * log(prob,2) #log base 2
    return shannonEnt
    
def splitDataSet(dataSet, axis, value):  #划分数据集
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting 降维，除去所选的划分属性的一维
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet
    
def chooseBestFeatureToSplit(dataSet):  #选择最好的数据集划分方式
    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels
    baseEntropy = calcShannonEnt(dataSet)  #原始香农熵
    bestInfoGain = 0.0; bestFeature = 0
    for i in range(numFeatures):        #iterate over all the features
        featList = [example[i] for example in dataSet]#create a list of all the examples of this feature
        uniqueVals = set(featList)       #get a set of unique values 从列表中创建一个集合（包含不同的元素）
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet)/float(len(dataSet))
            newEntropy += prob * calcShannonEnt(subDataSet)     
        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy计算信息增益
        if (infoGain >= bestInfoGain):       #compare this to the best gain so far
            bestInfoGain = infoGain         #if better than current best, set to best
            bestFeature = i
    return bestFeature                      #returns an integer
 
def majorityCnt(classList):  #多数表决法  
    classCount={}
    for vote in classList:
        if vote not in classCount.keys(): classCount[vote] = 0
        classCount[vote] += 1
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
 
def createTree(dataSet,labels):  #创建决策树
    labelnum = {'age':4,'workclass':7,'education':3,'marital-status':2,'occupation':3,'relationship':3,'race':2,'sex':2,'capital-gain':2,'capital-loss':2,'hours-per-week':3,'native-country':2}
    classList = [example[-1] for example in dataSet]
    if classList.count(classList[0]) == len(classList):
        return classList[0]#stop splitting when all of the classes are equal
    if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet        
        return majorityCnt(classList)    
    bestFeat = chooseBestFeatureToSplit(dataSet)  #选择最好的划分属性的index
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel:{}}
    del(labels[bestFeat])
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    if len(uniqueVals) == 1:
        return majorityCnt(classList) 
    elif len(uniqueVals) < labelnum[bestFeatLabel]:
        myTree[bestFeatLabel]["default"] = majorityCnt(classList) 
    for value in uniqueVals:
        subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)   
    return myTree                          
    
def classify(inputTree,featLabels,testVec):  #使用决策树的分类函数
    firstStr = inputTree.keys()[0]
    secondDict = inputTree[firstStr]
    featIndex = featLabels.index(firstStr)
    key = testVec[featIndex]
    if key not in secondDict:
        classLabel = secondDict["default"]
        return classLabel 
    valueOfFeat = secondDict[key]
    if isinstance(valueOfFeat, dict): 
        classLabel = classify(valueOfFeat, featLabels, testVec)       
    else: classLabel = valueOfFeat
    return classLabel
 
def storeTree(inputTree,filename):  #保存决策树
    import pickle
    fw = open(filename,'w')
    pickle.dump(inputTree,fw)
    fw.close()
    
def grabTree(filename): 
    import pickle
    fr = open(filename)
    return pickle.load(fr)
```

treePlotter.py
```python
# -*- coding: utf-8 -*-
'''
Created on Oct 14, 2010
@author: Peter Harrington
'''
import matplotlib.pyplot as plt
 
decisionNode = dict(boxstyle="sawtooth", fc="0.8") #文本框的格式，决策点
leafNode = dict(boxstyle="round4", fc="0.8")    #文本框的格式，叶子节点
arrow_args = dict(arrowstyle="<-")              #箭头的格式
 
def getNumLeafs(myTree):  #获取叶子结点的数目
    numLeafs = 0
    firstStr = myTree.keys()[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes
            numLeafs += getNumLeafs(secondDict[key])
        else:   numLeafs +=1
    return numLeafs
 
def getTreeDepth(myTree):  #获取树的深度   
    maxDepth = 0
    firstStr = myTree.keys()[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes
            thisDepth = 1 + getTreeDepth(secondDict[key])
        else:   thisDepth = 1
        if thisDepth > maxDepth: maxDepth = thisDepth
    return maxDepth
 
def plotNode(nodeTxt, centerPt, parentPt, nodeType): #文字，坐标，父结点坐标，结点类型
    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',
             xytext=centerPt, textcoords='axes fraction',
             va="center", ha="center", bbox=nodeType, arrowprops=arrow_args )
    
def plotMidText(cntrPt, parentPt, txtString):  
    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]
    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]
    createPlot.ax1.text(xMid, yMid, txtString, va="center", ha="center", rotation=30)
 
def plotTree(myTree, parentPt, nodeTxt):#if the first key tells you what feat was split on
    numLeafs = getNumLeafs(myTree)  #this determines the x width of this tree
    depth = getTreeDepth(myTree)
    firstStr = myTree.keys()[0]     #the text label for this node should be this
    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)
    plotMidText(cntrPt, parentPt, nodeTxt)
    plotNode(firstStr, cntrPt, parentPt, decisionNode)
    secondDict = myTree[firstStr]
    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes   
            plotTree(secondDict[key],cntrPt,str(key))        #recursion
        else:   #it's a leaf node print the leaf node
            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW
            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)
            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD
#if you do get a dictonary you know it's a tree, and the first element will be another dict
 
def createPlot(inTree):
    fig = plt.figure(1, facecolor='white')
    fig.clf()
    axprops = dict(xticks=[], yticks=[])
    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)    #no ticks
    #createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses 
    plotTree.totalW = float(getNumLeafs(inTree))
    plotTree.totalD = float(getTreeDepth(inTree))
    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;
    plotTree(inTree, (0.5,1.0), '')
    plt.show()
 
#def createPlot():
#    fig = plt.figure(1, facecolor='white')
#    fig.clf()
#    createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses 
#    plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode)
#    plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)
#    plt.show()
 
def retrieveTree(i):   #预先存储树信息
    listOfTrees =[{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}},
                  {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}
                  ]
    return listOfTrees[i]
 
#createPlot(thisTree)
```

run.py
```python
# -*- coding: utf-8 -*-
"""
Created on Wed Dec 13 10:27:08 2017
@author: ChenYing
"""
import trees
import treePlotter
import csv
from sklearn import tree 
import numpy as np
import matplotlib.pyplot as plt
 
 
def translate(filename):
    age = {'0-25':0,'25-50':1,'50-75':2,'75-100':3}
    csvfile = file(filename, 'rb')
    reader = csv.reader(csvfile)
    data = []
    for line in reader:    
        data.append(line) 
    csvfile.close() 
    
    new_data = []
    mark = 0
    for dataline in data:
        x = [0,0,0,0,0,0,0,0,0,0,0,0,0]
        if mark ==0:
            new_data.append(dataline)
            mark += 1
        else:
            agenum = int(dataline[0])
            if agenum>=0 and agenum<25:
                x[0] = age['0-25']
            elif agenum>=25 and agenum<50:
                x[0] = age['25-50']
            elif agenum>=50 and agenum<75:
                x[0] = age['50-75']
            elif agenum>=75:
                x[0] = age['75-100']
            
            x[1] = dataline[1]    
            x[2] = dataline[2] 
            x[3] = dataline[3]  
            x[4] = dataline[4] 
            x[5] = dataline[5]
            x[6] = dataline[6]
            x[7] = dataline[7]
            
            gain = int(dataline[8])
            if gain>0:
                x[8] = '>0'
            else:
                x[8] = '=0'
            loss = int(dataline[9]) 
            if loss>0:
                x[9] = '>0'
            else:
                x[9] = '=0'
            
            hour = int(dataline[10])
            if hour == 40:
                x[10] = '=40'
            elif hour > 40:
                x[10] = '>40'
            elif hour < 40:
                x[10] = '<40'
                
            if dataline[11] == 'United-States' :
                x[11] = 'USA'
            else:
                x[11] = 'not USA'
                
            if dataline[12] == '<=50K':
                x[12] = '<=50K'
            else:
                x[12] = '>50K'
            new_data.append(x)
    return new_data
    
 
def translateToValue(filename):  #把数据集转换成数值型的
    age = {'0-25':0,'25-50':1,'50-75':2,'75-100':3}
    capital_gain = {'=0':0, '>0':1} #10
    capital_loss = {'=0':0, '>0':1} #11
    hours_per_week = {'=40':0, '>40':1, '<40':2} #12
    native_country = {'USA':0, 'not USA':1} #13
    workclass=  {'Freelance': 1, 'Other': 3, 'Proprietor': 4, 'Private': 2, 'Government': 0}
    education=  {'Primary': 2, 'Tertiary': 0, 'Secondary': 1}
    maritial_status=  {'1': 1, '0': 0}
    occupation=  {'High': 1, 'Med': 2, 'Low': 0}
    relationship=  {'Other': 0, 'Husband': 1, 'Wife': 2}
    race=  {'1': 0, '0': 1}
    sex=  {'Male': 0, 'Female': 1}
    income = {'<=50K':0, '>50K':1}
 
    csvfile = file(filename, 'rb')
    reader = csv.reader(csvfile)
    data = []
    for line in reader:    
        data.append(line) 
    csvfile.close() 
    
    new_data = []
    mark = 0
    for dataline in data:
        x = [0,0,0,0,0,0,0,0,0,0,0,0,0]
        if mark ==0:
            new_data.append(dataline)
            mark += 1
        else:
            agenum = int(dataline[0])
            if agenum>=0 and agenum<25:
                x[0] = age['0-25']
            elif agenum>=25 and agenum<50:
                x[0] = age['25-50']
            elif agenum>=50 and agenum<75:
                x[0] = age['50-75']
            elif agenum>=75:
                x[0] = age['75-100']
            
            x[1] = workclass[dataline[1]]     
            x[2] = education[dataline[2]]  
            x[3] = maritial_status[dataline[3]]  
            x[4] = occupation[dataline[4]]  
            x[5] = relationship[dataline[5]]
            x[6] = race[dataline[6]]
            x[7] = sex[dataline[7]]
            
            gain = int(dataline[8])
            if gain>0:
                x[8] = capital_gain['>0']
            else:
                x[8] = capital_gain['=0']
            loss = int(dataline[9]) 
            if loss>0:
                x[9] = capital_loss['>0']
            else:
                x[9] = capital_loss['=0']
            hour = int(dataline[10])    
            if hour == 40:
                x[10] = hours_per_week['=40']
            elif hour > 40:
                x[10] = hours_per_week['>40']
            elif hour < 40:
                x[10] = hours_per_week['<40']
                
            if dataline[11] == 'United-States' :
                x[11] = native_country['USA']
            else:
                x[11] = native_country['not USA']
                
            if dataline[12] == '<=50K':
                x[12] = income['<=50K']
            else:
                x[12] = income['>50K']
            new_data.append(x)
    return new_data
 
def write_new_data():
    #adult_data_all在原始数据的基础上对某些属性做了一定的合并、修改等
    new_data_value = translateToValue('adult_data_all.csv')
    with open( './new_data_value.csv', 'wb') as f:
        writer = csv.writer(f)    
        writer.writerows(new_data_value)
    f.close()       
    
    new_data_value_test = translateToValue('adult_test_all.csv')
    with open( './new_data_value_test.csv', 'wb') as f:
        writer = csv.writer(f)    
        writer.writerows(new_data_value_test)
    f.close()   
    
    new_data = translate('adult_data_all.csv')
    with open( './new_data.csv', 'wb') as f:
        writer = csv.writer(f)    
        writer.writerows(new_data)
    f.close()       
    
    new_data_test = translate('adult_test_all.csv')
    with open( './new_data_test.csv', 'wb') as f:
        writer = csv.writer(f)    
        writer.writerows(new_data_test)
    f.close()   
 
def readData(filename):
    csvfile = file(filename, 'rb')
    reader = csv.reader(csvfile)    
    data_all = [] #训练数据集
    data_feature = [] #特征列
    data_label = [] #标签列
    mark = 0
    featurnlen = 0
    for line in reader: 
        if mark ==0:
            featurnlen = len(line) - 1
            mark += 1
        else:               
            data_all.append(line)
            data_feature.append(line[0:featurnlen])
            data_label.append(line[-1]) 
    csvfile.close()
    return data_all,data_feature,data_label
 
#调用sklearn的决策树函数
def use_sklearn_tree():
    train_data,trainX,trainY = readData('new_data_value.csv')
    test_data,testX,testY = readData('new_data_value_test.csv')
        
    model = tree.DecisionTreeClassifier() 
    
    model.max_depth = 8
    model.min_samples_split = 9
    model.fit(trainX, trainY)  
    predict = model.predict(testX)    
    
    accuratyNum = 0
    total = 0
    for index in range(len(predict)):
        if predict[index] == testY[index]:
            accuratyNum += 1
        total += 1 
    print "when use the sklearn............"
    importances =  model.feature_importances_
#    print "the accuratyNum is",accuratyNum
#    print "the total num is",total
    print "the accuraty is"
    accuracy = float(accuratyNum)/total
    print 'accuracy: %.2f%%' % (100 * accuracy) 
    return model.tree_
    
 
def use_myTree():
    adultLabels = ['age','workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']
    adultLabels_test = ['age','workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']
    adult = readData('new_data.csv')[0]
    adult_test = readData('new_data_test.csv')[1]
    adult_test_label = readData('new_data_test.csv')[2]
    adultTree = trees.createTree(adult,adultLabels) #生成决策树
    treePlotter.createPlot(adultTree)  #画出决策树
    
    predict = []  #预测的标签
    for i in range(len(adult_test)):
        predict.append(trees.classify(adultTree,adultLabels_test,adult_test[i]))
    accuratyNum = 0
    total = 0
    for index in range(len(predict)):
        if predict[index] == adult_test_label[index]:
            accuratyNum += 1
        total += 1   
    print "when use my tree..............."
#    print "the accuratyNum is",accuratyNum
#    print "the total num is",total
    print "the accuraty is"
    accuracy = float(accuratyNum)/total
    print 'accuracy: %.2f%%' % (100 * accuracy) 
    print " "
    return adultTree
 
#write_new_data()        
myTree = use_myTree()
sklearnTree = use_sklearn_tree()
#accuracy,importance = use_sklearn_tree(8,9)
 
#accuracy = np.zeros(18)
#print accuracy
#for i in range(18):
#    if i>=2:
#        accuracy[i] = use_sklearn_tree(8,i)*100
 
#x = np.arange(0, 18, 1)
#y = accuracy        
#plt.figure(1)
#plt.subplot(211)
#plt.axis([0, 18, 84, 84.25])
#plt.plot(x,y,marker='o',mec='r', mfc='w')
```


## 9.熟悉SPMF中贝叶斯文本分类算法（an algorithm for **classifying text documents** using a Naive Bayes classifier approach (S. Raghu, 2015)。
```java
package ca.pfv.spmf.algorithms.classifiers.naive_bayes_text_classifier;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.math.BigDecimal;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map.Entry;
import java.util.Set;
import java.util.TreeMap;

import ca.pfv.spmf.tools.MemoryLogger;
import ca.pfv.spmf.tools.textprocessing.PorterStemmer;
import ca.pfv.spmf.tools.textprocessing.StopWordAnalyzer;

/** * * * This is an implementation of the Naive Bayes Document Classifier algorithm. 
* 
* Copyright (c) 2014 Sabarish Raghu
* 
* This file is part of the SPMF DATA MINING SOFTWARE * (http://www.philippe-fournier-viger.com/spmf). 
* 
* 
* SPMF is free software: you can redistribute it and/or modify it under the * terms of the GNU General Public License as published by the Free Software * Foundation, either version 3 of the License, or (at your option) any later * version. * 

* SPMF is distributed in the hope that it will be useful, but WITHOUT ANY * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR * A PARTICULAR PURPOSE. See the GNU General Public License for more details. * 
* 
* You should have received a copy of the GNU General Public License along with * SPMF. If not, see . 
* 
* @author SabarishRaghu
*/
/**
 * 
 * Input can be of any format. 
 * For the training data, Please place the input files under corresponding ClassNames as FolderNames.
 * Output has the following format outputfileName \t ClassName 
 */
public class AlgoNaiveBayesClassifier {
	private String mTestDataDirectory="";
	private String mTrainingDataDirectory="";
	private boolean mInMemoryFlag=false; //By Default operations are executed by File IO rather than in memory objects.
	private HashMap<String,List<File>> mFileLists=new HashMap<String,List<File>>(); 
	private ArrayList<String> mClassNames;
	private StopWordAnalyzer mAnalyzer;
	private PorterStemmer mStemmer;
	private String mOutputDirectory="";
	private ArrayList<MemoryFile> mMemFiles=new ArrayList<MemoryFile>();
	long mStartTimestamp = 0; // last execution start time
	long mEndTimeStamp = 0; // last execution end time
	HashMap<String,Integer> classProb;
	/**
	 * 
	 * @param trainingDirectory
	 * @param testDirectory
	 * @param outputDirectory
	 * @param memoryFlag
	 * @throws Exception
	 */
	public void runAlgorithm(String trainingDirectory,String testDirectory,String outputDirectory,boolean memoryFlag) throws Exception
	{
		this.mTrainingDataDirectory=trainingDirectory;
		this.mTestDataDirectory=testDirectory;
		this.mOutputDirectory=outputDirectory;
		this.mInMemoryFlag=memoryFlag;
		this.runAlgorithm();
		Runtime.getRuntime().freeMemory(); //Mark the objects for GC's Mark & Sweep.
	}

	
	private void runAlgorithm() throws Exception
	{
		this.mStartTimestamp=System.currentTimeMillis();
		mAnalyzer=new StopWordAnalyzer();
		mStemmer=new PorterStemmer();
		classProb=new HashMap<String,Integer>();
		BufferedWriter writer=new BufferedWriter(new FileWriter(new File(mOutputDirectory+"/output.tsv")));
		ArrayList<OccurrenceProbabilties> op=new ArrayList<OccurrenceProbabilties>(); //A cache to hold probability for already calculated words in each class
		File[] listOfTestFiles=new File(mTestDataDirectory).listFiles();
		File[] listOfTrainingFiles=new File(mTrainingDataDirectory).listFiles();
		mClassNames=new ArrayList<String>();
		int totalTrainingFiles=0;
		for(File f: listOfTrainingFiles)
		{
			mClassNames.add(f.getName());
			OccurrenceProbabilties oc=new OccurrenceProbabilties();
			oc.setClassName(f.getName());
			oc.setOccuranceMap(new HashMap<String,Double>());
			op.add(oc);
			File classTraining[]=new File(mTrainingDataDirectory+"/"+f.getName()).listFiles();
			mFileLists.put(f.getName(), (List<File>) Arrays.asList(classTraining));
			classProb.put(f.getName(), classTraining.length);
			totalTrainingFiles++;
		}
		
		if(mInMemoryFlag==true)
		{
			System.out.println("Loading Data in to memory.... May take a while depending upon the size of the data");
			loadIntoMemory();
		}
		
		for(File f:listOfTestFiles)
		{
			TreeMap<String,BigDecimal> probabilities=new TreeMap<String,BigDecimal>();
			System.out.println("---------------Computing for Test File:"+f.getName()+"-----------");
			for(String currentClass:mClassNames)
			{
				TestRecord testRecord=readOneTestFile(f);
				BigDecimal prob=new BigDecimal(""+1.0);
				for(String word:testRecord.getWords())
				{
					double termProbInClass=0.0;
					if(getFromExistingProbability(word,op,currentClass)!=0.0)
					{
						termProbInClass=getFromExistingProbability(word,op,currentClass);
					}
					else
					{
						if(mInMemoryFlag==true)
							termProbInClass=calculateProbabilityInMemory(word,op,currentClass);
						else
							termProbInClass=calculateProbability(word,op,currentClass);
						for(OccurrenceProbabilties oc:op)
						{
							if(oc.getClassName().equalsIgnoreCase(currentClass))
							{
								oc.getOccuranceMap().put(word, termProbInClass);
								break;
							}
						}
					}
				prob=prob.multiply(new BigDecimal(""+termProbInClass));
				}
				//P(Class/Doc)=P(Doc/class)*P(class)
				prob=prob.multiply(new BigDecimal(""+((double)classProb.get(currentClass)/(double)totalTrainingFiles)));
				probabilities.put(currentClass, prob);
			}
			Entry<String,BigDecimal> maxEntry = null;

			for(Entry<String,BigDecimal> entry : probabilities.entrySet()) {
			    if (maxEntry == null || entry.getValue().compareTo( maxEntry.getValue())>0) {
			        maxEntry = entry;
			    }
			}	
		System.out.println(f.getName()+"\t"+maxEntry.getKey());
		writer.write(f.getName()+"\t"+maxEntry.getKey()+"\n");
		}
		writer.close();
		this.mEndTimeStamp=System.currentTimeMillis();
	}
	
	/**
	 * Load the training data in to memory
	 */
	private void loadIntoMemory() throws IOException
	{
		for(String s:mClassNames)
		{
			List<File> classTraining=mFileLists.get(s);
			MemoryFile memfile=new MemoryFile();
			ArrayList<String> words=new ArrayList<String>();
			memfile.setClassname(s);
			for(File f:classTraining)
			{
				BufferedReader reader=new BufferedReader(new FileReader(f));
				String currentLine="";
				while((currentLine=reader.readLine())!=null)
				{
					currentLine=currentLine.replaceAll("\\P{L}", " ").toLowerCase().replaceAll("\n"," ");
					currentLine=currentLine.replaceAll("\\s+", " ");
					currentLine=mAnalyzer.removeStopWords(currentLine);
					for(String processedWord:currentLine.split("\\s+"))
					{
						processedWord=mStemmer.stem(processedWord);
						if(processedWord.length()>1)
						{
							words.add(processedWord);
						}
					
					}
				}
			reader.close();
			}
			memfile.setContent(words);
			mMemFiles.add(memfile);
		}
	}
	
	/**
	 * Get the probability value of a particular word in a particular class. Calculates them from in memory stored objects
	 * @param word
	 * @param op
	 * @param currentClass
	 * @return probability value of the word in the class
	 */
	private double calculateProbabilityInMemory(String word,ArrayList<OccurrenceProbabilties> op, String currentClass)
	{
		double prob=0.0;
		int count=0;
		int occurances=0;
		for(MemoryFile memFile:mMemFiles)
		{
			if(memFile.getClassname().equals(currentClass))
			{
				occurances+=Collections.frequency(memFile.getContent(), word)*50; //Giving more weightage to occurances rather than just incrementing by 1.
				count+=memFile.getContent().size();
			}
		}
		prob=(double)((double)occurances+50.0)/(double)((double)count+100.0); // Normalizing the value to avoid return of 0.0
		return prob;
	}
	
	/**
	 * Get the probability value of a particular word in a particular class. Calculates them from File search. 
	 * @param word
	 * @param op
	 * @param currentClass
	 * @return probability value of the word in the class
	 * @throws Exception
	 */
	private double calculateProbability(String word,
			ArrayList<OccurrenceProbabilties> op, String currentClass) throws Exception {

		double probability=0.0;
		List<File> classTraining=mFileLists.get(currentClass);
		ArrayList<String> words=new ArrayList<String>();
		double count=0.0;
		for(File f:classTraining)
		{	
			BufferedReader reader=new BufferedReader(new FileReader(f));
			String currentLine="";
			while((currentLine=reader.readLine())!=null)
			{
				currentLine=currentLine.replaceAll("\\P{L}", " ").toLowerCase().replaceAll("\n"," ");
				currentLine=currentLine.replaceAll("\\s+", " ");
				currentLine=mAnalyzer.removeStopWords(currentLine);
				for(String processedWord:currentLine.split("\\s+"))
				{
					processedWord=mStemmer.stem(processedWord);
				if(processedWord.length()>1)
				{
					words.add(processedWord);
				}
				if(processedWord.equalsIgnoreCase(word))
				{
					count+=20;
				}
				
				}
			}
			reader.close();
		}
		probability=(double)((double)count+50.0)/(double)((double)words.size()+100.0);
		return probability;
	}
	
	/**
	 * Get from cached probabilities.
	 * @param word
	 * @param probabilties
	 * @param className
	 * @return the cached probabilities
	 */
	public double getFromExistingProbability(String word, ArrayList<OccurrenceProbabilties>probabilties, String className)
	{
		double value=0.0;
		for(OccurrenceProbabilties op:probabilties)
		{
			if(op.getClassName().equals(className))
			{
				Set<String> myKeys=op.getOccuranceMap().keySet();
				for(String s:myKeys)
				{
					if(op.getOccuranceMap().get(s) != null&&s.equals(word))
					{
						value=op.getOccuranceMap().get(s);
					}
				}
			}
		}
		
		return value;
	}
	
	/**
	 * Reads one test file and stores it in Object.
	 * @param f
	 * @return
	 */
	public TestRecord readOneTestFile(File f) throws Exception
	{
		TestRecord record=new TestRecord();
		String currentLine;
		ArrayList<String> words=new ArrayList<String>();
		BufferedReader br=new BufferedReader(new FileReader(f));
		while((currentLine=br.readLine())!=null)
		{
			currentLine=currentLine.toLowerCase(); //convert everyword to lower case
			currentLine=currentLine.replaceAll("\\P{L}", " "); //only alphabets
			currentLine=currentLine.replaceAll("\n"," ");
			currentLine=currentLine.replaceAll("\\s+", " ").trim();
			currentLine=mAnalyzer.removeStopWords(currentLine);
			String lineWords[]=currentLine.split("\\s+");
			for(String eachWord:lineWords)
			{
				String processedWord=mStemmer.stem(eachWord);
				if(processedWord.length()>1)
				{
				words.add(processedWord);
				}
				
			}
		}
		record.setRecordId(Integer.parseInt(f.getName().replaceAll("\\D+","")));
		record.setWords(words);
		br.close();
		
		
		return record;
	}
	
	/**
	 * Print statistics of the latest execution to System.out.
	 */
	public void printStatistics() {
		System.out.println("========== Naive Bayes Classifier Stats ============");
		System.out.println(" Total time ~: " + (mEndTimeStamp - mStartTimestamp)
				+ " ms");
		System.out.println(" Max memory:" + MemoryLogger.getInstance().getMaxMemory() + " mb ");
		System.out.println("=====================================");
	}
}
```

How to use this algorithm
An example of how to use the algorithm is provided in the file MainTestTextClassifier of the package ca.pfv.spmf.test. To run the algorithm, one needs to create an instance of the algorithm and call the runAlgorithm() method:

```java
AlgoNaiveBayesClassifier nbClassifier=new AlgoNaiveBayesClassifier();
nbClassifier.runAlgorithm(<Training_Directory>,<Test_Directory>,<Output_Directory>,<Memory_Flag>);
```

The output is a file indicating the categories of each text from the testing set.

Output ‘output.tsv’ would be found in the output directory ‘output.tsv’


- [CSDN-决策树如何处理取值为连续值的特征](https://blog.csdn.net/qq_43657442/article/details/109224662)
- [百度百科-过拟合](https://baike.baidu.com/item/%E8%BF%87%E6%8B%9F%E5%90%88/3359778)
- [知乎-理解过拟合](https://zhuanlan.zhihu.com/p/38224147)
- [CSDN-分类器评估方法](https://blog.csdn.net/weixin_37804469/article/details/105626206)
- [百度百科-查全率与查准率](https://baike.baidu.com/item/%E6%9F%A5%E5%85%A8%E7%8E%87%E4%B8%8E%E6%9F%A5%E5%87%86%E7%8E%87/424630)
- [简书-ROC曲线的概念和意义](https://www.jianshu.com/p/535301726e77)
- [知乎-机器学习笔记：朴素贝叶斯分类器](https://zhuanlan.zhihu.com/p/26055646)
- [SPMF-Classifying Text Documents Using A Naive Bayes Approach](http://www.philippe-fournier-viger.com/spmf/TextClassifierNaiveBayes.php)
- [sklearn中文文档-朴素贝叶斯](https://www.sklearncn.cn/10/)
- [CSDN-基于决策树和朴素贝叶斯算法对Adult数据集分类](https://blog.csdn.net/hohaizx/article/details/79084774)
